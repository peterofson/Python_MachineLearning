{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Linear Regression - Ridge Regression   \n",
    "-------------------------\n",
    "\n",
    "_Authors: Khal Makhoul, W.P.G.Peterson_  \n",
    "\n",
    "## Project Guide  \n",
    "\n",
    "--------------------\n",
    "- [Project Overview](#overview)  \n",
    "- [Data Review](#data)  \n",
    "- [Coding Ridge Regression](#code)\n",
    "- [Ridge Regression in `sklearn`](#sklearn)  \n",
    "\n",
    "<a id = \"overview\"></a>\n",
    "## Project Overview  \n",
    "-------------\n",
    "#### EXPECTED TIME 2.5 HRS  \n",
    "\n",
    "This assignment will test your ability to code your own version of ridge-regularized regression in `Python`. This assignment draws upon and presupposed the knowledge found in the lectures for Week 2. If ever a theoretical questions arises as to \"why\" we are doing something, please refer back to those lectures.  \n",
    "\n",
    "The assignment also builds upon the work performed in assignment 1 \"*Linear Regression - Least Squares*\". The data used will be the same. Though the last assignment tested your ability to read data into `Pandas` from a `.csv`. Those fundamental processes will not be directly tested here.\n",
    "\n",
    "In coding Ridge Regression you will be asked to:  \n",
    "- Mean center target variable and mean center / standardize observation  \n",
    "- Calculate Ridge Regression weights using linear algebra\n",
    "- Create a  hyperparameter tuning process   \n",
    "\n",
    "**Motivation**: Ridge Regression offers a way to mitigate some of the weaknesses of Least Squares Linear Regression to build more robust models.  \n",
    "\n",
    "**Objectives**: This assignmet will -  \n",
    "\n",
    "- Test `Python` competency and mathematical understandings of Ridge Regression\n",
    "- Begin to introduce the concept of hyper-parameter tuning  \n",
    "\n",
    "**Problem**: Using housing data, we will attempt to predict house price using living area with a regression model.  \n",
    "\n",
    "**Data**: Data comes from [Kaggle's House Prices Dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).  \n",
    "\n",
    "See above link for Description of data.\n",
    "\n",
    "<a id = \"data\"></a>  \n",
    "### Data Exploration\n",
    "\n",
    "Below provides a review of the \"Housing\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### This cell imports the necessary modules and sets a few plotting parameters for display\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)  \n",
    "\n",
    "### Read in the data\n",
    "tr_path = '/Users/Stellarhardwood/Downloads/Assignment 1 - datasets/train.csv'\n",
    "test_path = '/Users/Stellarhardwood/Downloads/Assignment 1 - datasets/test.csv'\n",
    "\n",
    "data = pd.read_csv(tr_path)  \n",
    "\n",
    "### The .head() function shows the first few lines of data for perspecitve\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Which column has the most \"null\" values? assign name as string to ans1.\n",
    "### ### CAPITALIZATION/SPELLING MATTERS e.g. 'Street' != 'street'\n",
    "### How many nulls are in that column? assign number as int to ans2\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = ''\n",
    "ans2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 1",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsnXmYVdWV6H+r5pmZAoGiGIXCDMooJaiAyigmrR2VVgIm9utWTF5i0mpAXwtqOnME27StOAU1xkRlUJFBZVIR0ShQKMUglDJaA0XNRa33xz33cucaqHtrYP2+7373nHX2OXsfuHXW2WvaoqoYhmEYRiSJaekBGIZhGO0fUzaGYRhGxDFlYxiGYUQcUzaGYRhGxDFlYxiGYUQcUzaGYRhGxDFlYxiGYUQcUzaGYRhGxDFlYxiGYUScuJYeQGuha9eump2d3dLDMAzDaFN8+OGHJ1S1W33tTNk4ZGdns23btpYehmEYRptCRL5oSDszoxmGYRgRx5SNYRiGEXFM2RiGYRgRx5SNYRiGEXFM2RiGYRgRx5SNYRjtguLyatyLQaoqxeXVLTwiwxtTNoZhtHmKy6u5eslmFq7MQ1VZuDKPq5dsNoXTirA8G8Mw2jwdkuOZNDSTpZv3s3TzfgDm5vajQ3J8C4/McGMzG8Mw2jwiwoLpQ31kC6YPRUQC2pq5rWUwZWMYRpvHbTrzxm1S88bMbS2HKRvDMNo8JRU1rM07ytzcfux/aCpzc/uxNu8oJRU1Pu28zW397n6NpZv3M2loppnbooD4a/5zlREjRqjVRjOMtktxeTUdkuMREVSVkooaOqYkBLRTVfrd/Zpnf/9DU4Oa24yGISIfquqI+trZzMYwjHZBx5QEj9IQkZCKpiHmNqP5MWVjGMY5Q0PNbUbzY2Y0BzOjGca5QUPNbUbDaKgZzfJsDMM4p/BWLKHMbUbzY2Y0wzAMI+KYsjEMwzAijikbwzAMI+KYsjEMwzAiTsSUjYicLyIfe31OisiPRaSziKwRkT3OdyenvYjIwyKSLyKfiMhFXtea7bTfIyKzveTDReRT55yHxQmyD9WHYRiG0TJETNmo6meq+m1V/TYwHCgHXgbuAtap6iBgnbMPMAUY5HxuBR4Fl+IA7gNGA6OA+7yUx6NOW/d5kx15qD4MwzCMFiBaZrSJwF5V/QKYCTztyJ8GrnG2ZwLPqIv3gI4i0hO4ClijqoWqWgSsASY7xzJU9V11JQs943etYH0YhmEYLUC0lM31wPPOdqaqHgZwvrs78l7AIa9zChxZOHlBEHm4PnwQkVtFZJuIbDt+/HgTb80wDMOoj4grGxFJAK4G/lpf0yAybYK8wajqY6o6QlVHdOvWrTGnGoZhGI0gGjObKcB2VT3q7B91TGA438cceQHQx+u83sBX9ch7B5GH68MwDMNoAaKhbG7gjAkNYDngjiibDbzqJb/ZiUobA5Q4JrDVwJUi0skJDLgSWO0cKxWRMU4U2s1+1wrWh2EYhtECRLQ2moikAFcA/+ol/iXwoojcAhwErnPkrwFTgXxckWtzAFS1UEQWAh847e5X1UJn+9+Ap4Bk4HXnE64PwzAMowWwqs8OVvXZMAyj8djiaYZhGEarwZSNYRiGEXFM2RiGYRgRx5SNYRiGEXFM2RiGYRgRx5SNYRiGEXFM2RiGYRgRx5SNYRiGEXFM2RiGYRgRx5SNYRiGEXFM2RiGYRgRx5SNYRiGEXFM2RiG0WCKy6txF+9VVYrLq1t4REZbwZSNYRgNori8mquXbGbhyjxUlYUr87h6yWZTOEaDiOh6NoZhtB86JMczaWgmSzfvZ+nm/QDMze1Hh+T4Fh6Z0RawmY1hGA1CRFgwfaiPbMH0obgWyjWM8JiyMQwH80eEx20688ZtUjOM+jBlYxiYP6IhlFTUsDbvKHNz+7H/oanMze3H2ryjlFTUtPTQjDZARJeFFpGOwOPABYACc4HPgL8A2cAB4J9VtUhcc/E/AlOBcuD7qrrduc5sYL5z2UWq+rQjHw48BSQDrwE/UlUVkc7B+gg3VlsW+tzGrWDcvghw+SPMTORLcXk1HZLjERFUlZKKGjqmJLT0sIwWpLUsC/1H4A1VHQJ8C8gD7gLWqeogYJ2zDzAFGOR8bgUeBXAUx33AaGAUcJ+IdHLOedRp6z5vsiMP1YdhBMX8EQ2jY0qC599EREzRGA0mYspGRDKA8cATAKpararFwEzgaafZ08A1zvZM4Bl18R7QUUR6AlcBa1S10JmdrAEmO8cyVPVddU3PnvG7VrA+DCMo5o8wjMgSyZlNf+A48KSIfCQij4tIKpCpqocBnO/uTvtewCGv8wscWTh5QRA5YfowjKCYP8IwIksk82zigIuAear6voj8kfDmrGD2Cm2CvMGIyK24zHBkZWU15lSjndExJYHlt+d6/BELpg/ljokDzUxkGM1EJGc2BUCBqr7v7L+ES/kcdUxgON/HvNr38Tq/N/BVPfLeQeSE6cMHVX1MVUeo6ohu3bo16SaN9kM0/REWZm2ca0RM2ajqEeCQiJzviCYCu4DlwGxHNht41dleDtwsLsYAJY4JbDVwpYh0cgIDrgRWO8dKRWSME8l2s9+1gvVhGC2OhVkb5yKRLlczD1gmIgnAPmAOLgX3oojcAhwErnPavoYr7DkfV+jzHABVLRSRhcAHTrv7VbXQ2f43zoQ+v+58AH4Zog/DaHGs7ItxLhLRPJu2hOXZGNFEVel392ue/f0PTbUwa6NN0lrybAzD8MPCrI1zEVM2hhFlLMzaOBcxM5qDmdGMaGJlX4z2QkPNaLaejWG0AN6Kxcq+GOcCZkYzjCBEKw/G8m2McwVTNobhR7TyYCzfxjiXMDOaYfgRrTwYy7cxziVsZmMYfkRruQFb1sA4lzBlYxh+RCsPxvJtjHMJUzaG4Ue08mCi0Y8FIBitBcuzcbA8G8ObaOXBRLIfdwDCpKGZLJg+lIUr81ibd5Tlt+daqLXRbFiejWGcBdHKg4lkPxaAYLQmzIxmGO0UC0AwWhOmbAyjnWIBCEZrwpSNYbRTrOCn0ZqwAAEHCxAw2iNW8NOINBYgYBiGFfw0Wg1mRjMMwzAijikbwzAMI+JEVNmIyAER+VREPhaRbY6ss4isEZE9zncnRy4i8rCI5IvIJyJykdd1Zjvt94jIbC/5cOf6+c65Eq4Pw7CMesNoGaIxs7lcVb/t5UC6C1inqoOAdc4+wBRgkPO5FXgUXIoDuA8YDYwC7vNSHo86bd3nTa6nD+Mcxkr6G0bL0RJmtJnA087208A1XvJn1MV7QEcR6QlcBaxR1UJVLQLWAJOdYxmq+q66XlWf8btWsD6McxjvjPp+d7/G0s37mTQ00zLqDSMKRFrZKPCmiHwoIrc6skxVPQzgfHd35L2AQ17nFjiycPKCIPJwffggIreKyDYR2Xb8+PEm3qLRVrCMesNoOSKtbHJV9SJcJrLbRGR8mLbB/uK1CfIGo6qPqeoIVR3RrVu3xpxqtEHaY0a9+aCMtkJElY2qfuV8HwNexuVzOeqYwHC+jznNC4A+Xqf3Br6qR947iJwwfRjnMO0to958UEZbImLKRkRSRSTdvQ1cCewAlgPuiLLZwKvO9nLgZicqbQxQ4pjAVgNXikgnJzDgSmC1c6xURMY4UWg3+10rWB/GOUzHlASW357rMZ0tmD60TZfbNx+U0ZaI5MwmE9gkIv8AtgKrVPUN4JfAFSKyB7jC2Qd4DdgH5AP/C/w7gKoWAguBD5zP/Y4M4N+Ax51z9gKvO/JQfRjnOB1TEjw+mraeUR9tH5SZ7IyzwWqjOVhtNKOt4TadudeqAdd6NZFQOLYQmxGKhtZGswoChtFMRPvNP5o+KDPZGWeLzWwcbGbT9mnJCsct9eYfzXtWVfrd/Zpnf/9DUy1s3Gj+mY2IXCIic5ztbiLS72wGaBjNSUtHZrXUm3+0fFDtMWzciC4NUjYich/wH8Ddjige+HOkBmUYjaWlzTztPWG0vYWNG9GnoTOb7wBXA2XgyZ9Jj9SgDKOxNPVh31x+lvb+5t/ewsaN6NNQZVPt1B9T8OTNGEaroSkP++Y0vZ0Lb/7tKWzciD4NXanzRRH5H1zFMX8IzMWVC2MYrQLvh723g/6OiQNDPhS9TW/u8OG5uf2aZHpzv/m7nfULpg9l9ti+nmvZkszGuU6Do9FE5Apc2fuCK4N/TSQHFm0sGq3t05TIrEhFWFleinGu0KzRaE7k2UZV/Zmq3omrMkD22Q3RMJpGKD9LY808kfSzBA9Y6N6ga1umvtEeaajP5q9Andf+aUdmGFGlrfhZRIR5Ewb4yFRh5iNbwo61pUO4DSNSNNRnE6eqnl+7qlaLiNkCjKgTaT9LOB9PY1BVFq/P95E9ueUAc8Zmhx1rc9xfSya3GkYoGjqzOS4iV7t3RGQmcCIyQzKM0DR3PkukIqxcs6ZjzM3N9pHfMXFg2LGe7f3ZzMhorTRU2fwf4B4ROSgih3AleP5r5IZlGMGpz8/SVH9Hc/tJOqYk8OptY/F30Sxev9dz/WD9na0fqaWTWw0jFA1SNqq6V1XHADlAjqqOVdX8+s4zjOYmnJ+lqW/1kZoNiAjrdh8LGOvBwvKQ/Z2tH6m9VzIw2i5hQ59F5F9U9c8i8pNgx1X1dxEbWZSx0Oe2QyifRKiS+/OnDeFkZW1IE1kkS/UHG2uH5Piw/Z2NzyWayw4YBjRf6LO7UkB6iI9hRJ1QfpZgb/Xzpw1h0ardYWcqzT0b8DaRdUiO98xK3GOtr7+z8SOdC5UMjLZJ2Gg0Vf0fEYkFTqrq76M0JsNoEsH8Hf3vcS3eGi6iK5SfpCkKpyHJnM3Znz/uCDu3slswfWhACLZhtAT1+mxU9TSuIpyG0arxfqvf9+AUn2PhHuTNORtoiIM+GrOPmY9s8Si0xev3WkSa0eI0qFyNiDwAdAD+glP5GUBVtzfg3FhgG/Clqk53qhG8AHQGtgM3OXk7icAzwHDga+B7qnrAucbdwC24kknvUNXVjnwy8EcgFnhcVX/pyIP2EW6c5rNpm/j7Nw4WltOnUzKLVu1ulN8ilJ+kuUvguK9XUlFDRlIcJytrPfvNFXZtfhsjmjT34mljgWHA/cBvnc9vGnjujwBvm8F/Ab9X1UFAES4lgvNdpKoDgd877RCRHOB6p//JwH+LSKyjxB4BpuCKkrvBaRuuD6MdESyK7KYntnKoqKLRM4dgfpKmRKmFC132vl6H5HiPL6m5ky4tIs1ojTSogoCqXt6Ui4tIb2Aa8ADwE3H92icANzpNngb+H/AoMNPZBngJWOK0nwm8oKpVwH4RyQdGOe3yVXWf09cLwEwRyQvTh9FGCDajKKmooU+nZE5W1pKeGEtxeTUTh3QPyLbP6pzSLJUBmpLNH676dHNWPwhHJH1ChtFUwiobERkNPAYMAD4F5qpqXrhz/PgD8HPORK51AYpVtdbZLwB6Odu9gEMAqlorIiVO+17Ae17X9D7nkJ98dD19GM1AU0NzG3peMCf7658e5tipKgZ1T+NUZQ3lNXWcrKjhmm/39DnX/UD1vm5TKwO4FZW3Oaq+B3Z9JXAae72m0JTlFgwj0tRnRnsEuBPXA/x3uJRHgxCR6cAxVf3QWxykqdZzrLnkwcZ4q4hsE5Ftx48fD9bE8CMaiZPBnOxXDctkSGY6u4+UUlBcSWFZNbV1ykvbv/I5tzmqNrtDl13j3NXo64cKXY7Wap62qqbRGqlP2cSo6hpVrVLVvwLdGnHtXOBqETmAy1k/AZey6igi7hlVb8D9tCgA+gA4xzsAhd5yv3NCyU+E6cMHVX1MVUeo6ohu3Rpza+cuTS2H0pjzglVM/tGkQayYlxv02nPGZrP/oanMGpXFml1HKKmoaXLJGW+lWFxezV8/LCA9MY6PFkxqVNRYsHI00cyBsVU1jdZGfcqmo4h81/0Jsh8SVb1bVXurajYuB/96VZ0FvAVc6zSbDbzqbC939nGOr3eWol4OXC8iiU6U2SBgK/ABMEhE+jkVqK8HljvnhOrDOEua6nxuzHlFZVWM/9XbPrJLfrmeqQ9vDHFt18N9Y/4Jxg/u5snQb0q4r7dSvHDhWkora7l2eG86piTUO0NwK5ji8mpmLN7E/Jd3+MzigFY747A1dIxIU5+yeQeY4fXx3p/exD7/A1ewQD4u89wTjvwJoIsj/wlwF4Cq7gReBHYBbwC3qeppxydzO7AaV7Tbi07bcH0YZ0lTTUGhzisqqwr+kPPTQQrsOVbGkB7p9O6YRKeUeOJihKu/2ZM1u44CMHFId5a9f/CsClAGU4r3zshBRMLOELxnRBlJcWQkxbNsa+BYWuOMwypFG9GgvgoCc5qjE1V9G3jb2d7HmWgy7zaVwHUhzn8AV0Sbv/w14LUg8qB9GGdPU53P/ufNf2UHq3ce4c1dR7gyJ5N5Ewbymzc/Z+OeEyy/PZd37ryUixat85zfKTWRi/t35r/+6Rvc++ou3v7sKI/cOJK7/r6DC7M6kpEUh/8kqSnO96ZGcgWLNDvbsUSLaEXJGec2DU3qzAQeBM5T1SlOPsvFqtpuZgyW1NlwzjYaraSihhmLNzFuYFeSEmJZuvmAp01OzwyenTuCm5duY+fhkx75sJ4ZPvvuAps//svHLP/H4aD9NSWRsSHlZkLhn8x5tmOJJuESUQ0jHM2d1PkULnPVec7+58CPmzY0o63TVFOQ+zxVZdLQTJ774JCPogHYdfgkwx9Yz87DJxnWM4N9D05hbm4/Tlb6OtEXTB/Kycpatn9RFNDP+d3TmJOb3STne1MjuYLNiLzH35qLYUYrSs44t2mosumqqi8CdeDKg8FVOsYwGkVxeTUzH9kSYPIKxop5ucTExDB/2hDGD/aNFly4Mo/0xFg6JAcqgVV35HLv9JwmO98bo0zdjvWSihrW7DrCrFFZrsi40VmUVtVysrK2VQUCBMMqRRvRoKHKpkxEuuDkq4jIGKAkYqMy2i0u/0D3gBlNMBat2o2qcrKylo17TjBrdJbPTGHX4VJKqwIfiNMWb0FVI/5w9y8/M35wNzbmn6CkooZF11zgUTDRCgRoakSZ5eUY0aChyuYnuEKQB4jIZlwFM+dFbFRGu8WVQzPQRzY0M81nf25uNnPGnjGDdUxJ4NlbRrHh8+MsWrWb+dOGMG5gV/592XaG9ezgc27HlHjyj5/iUFFFs4051EPcP3do2fsHPVFn0Y40O9uIstYYJWe0Lxq6LPR24FJcBTn/FRimqp9EcmBG+0RVWbx+r4+soLiSG0f1cRIns1mbd4w7Jg70ebvO6pzCFTk9WLp5P/3veZ1lWw8yflA3dh4+ybXDe7H3gcnMze1HemIcz84dSVbnFE9/7gduU978wz3EW1PBy6Ym2xpGtKhvWej6Ejf/3uwjaiEsGi06BIv2enPXEVbcnkun1MSw0W3BIqbcyyx7LzFw0xNbA6LJnr1lVFB5MHORd7RdXV0dC17ZybKtBz3H3ZFlQKsq5W8RZUZL0NBotPqqPs8Ic0yBdqNsjOhQX6HKUCachuS/iAhZnVOC5oz06ZQcMpfEW7kUlVUxY8lmrszpwYLpQ1m0ajcb80/49Ovus7i8mrV5R5k1OouFM4exaNVu1uYdZfbYvvTtktrkEPGmYJWejdZOWDOaqs4J85kbrUEa7Yum+AcaEjHlNosFM23FxMQElZdU1PiYyR5el09JeY2POSo5zvfPxF2GJpQv6aYntvLF12VRzcq3iDKjtdOgpE4AEZmGawGzJLdMVe+P0LiijpnRoktT3vrDneM2z00c0h0RfKLd3AmgwVbvDCb//sV9eerdL3z67pQST+3pOipr66irU9bfeRl9u6QGXRVzzths5k0YwJK39kXVxBbNmZRhuGnWpE4R+RPwPVwRaIKrrEzfsxqhcc7S2Mgpt2PfW7H4z4jcDvIntxzwKJo5Y7OZm5vN6p1HOFhYztq8o8wZm+1TwdmdB+PNBwcCE0WLymsorTpNzWnln0f0JqtziueBfvPFWT5tq2tPc81/vxtQuTrSJi2LKDNaMw1eFlpVb8a1bPN/AhfjW97fMBpMh+R4xg3q6mOqGjeoa9DIqYYqplAFNOdNGIgIPPPuQV69bSwiMH3xZm6/vD/Lb88lIymO+S/v8DnPuyxOMJITXK7OhSvzmPbwJqYv3uRzfNnWQ1wysAsPr8v3kVtWvnEu01Bl405aKBeR84BaoF9khmS0V7yz7d/57JjPsQ2fHw/wv6iqVxJo+JDecCVXrsw5s2TA0s0HOFlZw5K39tIhOZ4Fr+7kL9sOeTL/bxxZ/zvU0s0HPGO5dHDXoG2+N6I363Yf8/GhvLnriEdJWhl/41yjocpmpYh0BH4FfAjsx7UgmmE0CO8ZSnpiLGXVvtWOUhPjyEiKC2gL4D8ZCGaOCuYgf3PXEWYs2RJw/tXf6ulRGMveP8j3RvZh4TXDKC6vZsUngUU9UxNiOb97GgLEx/j2+7OrzmfF7YGLuv3L0g94Zu5Iz1jnTRiAKixev9fK+BvnJGFDn0VkJHBIVRc6+2nAp8Bu4PeRH57RXqivBP/nR0vZ8WUJWY7TfeKQ7iHbzn9lB4uuucBH4QQLqZ43YQAPr8vnyS0HfM5PiPV9x/K5lp9LJS0hlmU/GEWvjkn89s3PeX7blz7HH163h6rauoAxap16xuIe31XDekSljL8FChitkfpmNv8DVAOIyHjgl46sBHgsskMz2guhQpK9SUuMY9bjW5nyh41M+cNGKmtqfY53Somnd8ckhvRI54WtB/ni6zLA1xzl7SB3P2DvmOhbGmfO2L689KGvwnCb2zqlJrLhZ5f5HFsxL5d/W/YRl/92I/6aaNboPry56xgb9pxg1ug+Aee5c3FUFRFh/rQhPm0iETBgC6EZrZX6kjpjVbXQ2f4e8Jiq/g34m4h8HNmhGe0B/5DkUJysdCmXSUO68so/jvD8BwU+x4vKaygqrwEqSUuM5Zl3v+D2ywfwmzc/Z1P+147z3xWB5e5z3MAuxPvNYqpq6uiQEs91I/oELP7WITme36z+3Kf9bc99TGlVLaVVtTy/zXdM8THCsh+MokNyPL9e/ZnPsduf+5jiimpAuGpYD+ZPG8KMxZt92kQi6dIWQjNaK/WVq9kBfFtVa0VkN3Crqm5wH1PVC6I0zohjeTaRIVgeyg0jewcoEzc3jurDtgOFfH6sLOQ1c3pmsMsrYuzGUX1Iio9lbd4xjylt/ss7fErMDOuZwcHCcjKS43nuh6PJ6pwSYGYqLq9mxuJNZCTF+0Sk3TiqD89tPeQzBvcMqUNKPMt+MJqbnthKemKcz3mzRvUhIS7Wx4w3rGcGK+bleqoNRKK6spWtMaJJQ/Ns6lM2vwCmAieALOAiVVURGQg8raqBntEz5yYBG4BEXDOol1T1PhHphyu4oDOwHbhJVatFJBFXNenhwNfA91T1gHOtu4FbcK2hc4eqrnbkk4E/ArHA46r6S0cetI9w/xCmbCKH/8MvVuB0BCKAZ43O8vhf6urq6H/P6z7H5+ZmM2/CQDqlJob0axSXV5ORFOdz7tAe6eQdKQ3apztR82BhOb07JjHgF294ju1/aCqAz73ve3AKMTExEfOlBFPurX2VUKNt0yxJnar6APBTXCt1XqJnNFMM9S8xUAVMUNVvAd8GJjvr4PwX8HtVHQQU4VIiON9FqjoQV/DBfzk3kgNcj6t6wWTgv0UkVkRigUeAKUAOcIPTljB9GFEmWEhyKEWTnhjaqnt+9zT6dErmxlG9Q7Zxh0+rKotW7Q44Pn/aUI+iCeXX6JAcH3Cut6IZ2iPd55i75M1NT2zl6iVbfPt7+VPufNHX2vzzlz6hqKwKcPmYmjsE2srWGK2VekOfVfU9VX1ZVcu8ZJ87yw6EO09V9ZSzG+98FJgAvOTInwaucbZnOvs4xyeK61VsJvCCqlap6n4gHxjlfPJVdZ8za3kBmOmcE6oPI8qUVNSweucR5ozNZv9DU5kzNpse6YlB22ZmJPnsn5+ZRnysIMCfbrqI5bfngoZ+O09NcIVPu1fNHNYzw+f4gld3euXuBC/HH/iwzva5xpgBXXz2F67MIyMpjnGDuvqY0Ib1zGDtrqP87aOv6JyawN4HJtM5NYGXtn/JlD9uipgD3xZCM1orDc2zaRLODORj4BiwBtgLFDvLSgMUAL2c7V7AIfAsO10CdPGW+50TSt4lTB9GCyAC1adPu7LnBUqragPaxMXAvhOnGOI1c/js6CmuG96bt+68lH7d0hERNu39mhtH9uHGkYEznPIa1zLMHVMS+PMPRlNaVcusUVnsfWAys0ZnseHz4xwsLA+7Do33wxoCc3xe+rCAubnZPrOGk5W1LLrG1325Yl4ur/94HEN6pFNYVs2AX7xBYVk152emMeUbPSK67szZlK1p6mqfhlEfEVU2qnpaVb8N9MY1EwkW++r+cw72yqrNKA9ARG4VkW0isu348ePBmhhnSYfkeC4d3I1l7x+i/z2v8+TmA0HzUhLjY3l27khWzfN1Az7wnW+Q3dW1kqdbETzw3W+w6DvfCLjGuIHdPA/tvl1SefaWUWzMP8EDr33GwpnDGD+4Gzc9sZWisirmv+Jbomb+Kzs8D1n3w7qkoiagCkBaQhwzv9UTVWX+tCE8PWcExeXVAabCRat20yk1kdfuuMRH/vqPxnHv9BwfWWvxp1jYtBFJIqps3KhqMfA2MAboKCJu43xv4CtnuwCn3ppzvANQ6C33OyeU/ESYPvzH9ZiqjlDVEd26dTubWzT8KC6v9vgm7r/a9+FaW6ec77cU9Kp5l3DX33cE+j28lADgeUtf8OpOn3bDemawYY9vyRvvtW363/O6Z9nmkooaXth6kJwe6ex7cAo5Tu7OwcJynzf7DsnxPHvLKL8qAMo1j77LjMWbuX/FLq78w0Ym/m4Db+w4HOAnKSqrCgh3nv7wJu5fuSvgHgtPVaKqqCpFZVUt8oC31T6NSBIxZSMi3ZwSN4hIMjAJyAPeAq51ms0GXnW2lzv7OMfXOwEJy4HrRSTRiTIbBGwFPgAGiUg/EUnAFUSw3DknVB9GMxPM7FJcXs30xZsY/6u3+c/lO5nmV6gSXCYybx57Zx/jBnYJ8Hts3HMiwLl9sLDuYgooAAAgAElEQVScDZ8fZ9Zol4ns2uG9KK2q4c8/GO1ZDA2CF+dcMH0oHZLjSU6IY9eRUvrf8zq7jpSSnBCHqga82d/0xFZKKmo8wQOTL+iBqqtY51PvfkHNaWVIZjor5wX6SUqrTvPZ0VKG9cxg34NTGNYzg8+OlrJ6xxGPYpo1OosXPjjE+F+9zf0rdrFw5S7G//ptpj28KeoKpzUtc220P+pL6jwbegJPO1FjMcCLqrpSRHYBL4jIIuAj4Amn/RPAsyKSj2tGcz2Aqu4UkReBXbgKgN6mqqcBROR2YDWu0Oelqup+3f2PEH0YzYj3Es/zJgxg8fp83tx1lGfmjGD8oK48t/VQwLow/szNzaay+jQvfljA9/z8MM/eMtLzoHM/7N2RX2P6d2HhzGEseHUnL3/0FdO+0YM+nZJ9loHOSIrjN2/6JlvOf2UHC2cO459H9PZZ88a9bECwhEhVZeYjW5g0NJN7Z+QE3NOzt4ykc9qZ4Aa3n6RjSgLrfnopfTolExMTw4p5uRwqqqBDcrwn7HrhzGGgyrKth3zyca69qHfUZxS22qcRSRq8eFp7x/JsGk+wnI7UxFiqak6TECOU19b/23rrp+Pp0ymZn/z1E5b/w7cI5vcv7ktMjPDGjiMATL6gJ7+Yej7THt7M7qNnwpHPz0zzmSkNzUyjpLKW2tOnOXaqhmE9M/hmrwz+ur0ARVj7f8fxvxv289wHZ+JL5ozN5t4ZLnOff0IkEHCf3sTHCu/ddTmxsbFNivryz0Ny9xvtB7z3y4N3dQWLZjPC0SxJnecSpmyaRrAHZWNIiRfO65jKvhOnqAuTf9OnUzK7QiRWfn9sX57a4jvbuH5EL1bvOkpRuW/k240je4MIz2895BM1kp4Yxzs/uzTk6ppAyPsU4LsXnscHXxQ3+sGsqsx/5VOWve9focCl/FpC4VgRT6MxNOtKnYYRjGBml2CEe1yW1yj5x0MrGoBrh/cOqWiAAEUD8MK2LwMUDcBzHxR4Ss/M+GYP9j04hbm52aQnxVFQVBGwmuebu45QVFYV9j4V+NtHXzFuYFfPMgkNDRs+WFjOCx8UEB8rzBnbl5yeGQiweueRFknEtNU+jUhhysZoMu4EyBtH9WFQt9SQ7c567hyBl/v+3VJY9ckRFryyk5vGZFFSUcOsx7fy9JwRVNWeZsaSzdx8cRaq8Ns397A27yjDemZ4Kk8HY2P+CRat2t2osOGszilcP6IPNaeVJ7d8wa7DJ7lhZB9W3XGJPeiNdoWZ0RzMjNY0vvi6jOv/512OnKwKUCqxQHwMVAam1dTL+ZnpPP/DUSx+ay9/3VbAqSCJoMGIEYLOkoZkprL7qG9xz5we6SFnTKkJsay4fSx/fr/Ax6w2a1QWG/YcZ/ygrkGLbHpH03nXYguHFc402jJmRjOiQlbnFCZ/o2fQ2ctpGqZoOiafCYrsmZHI1d/qSWlFDdMXb6ayupbU+BgGdU8NWzsNYGC31JDmOH9FA7D4hm+FvFZFzWmufmQLt1/e30e+6DsXsGLeJdx51fms233Mx+RW5DeLUYWZj2zxmd34h4oHM9G5Q68Noz1hysYIS33lS0SEOyYMDHZqw/uocM1aBndP5bkfjmbbgSJKKmvomBLP8x8UcPRUNXuOlZGZEXyGkN05mfSkOMqrT/Pnub4vWOt/Mi5kv5N+H5j/46ZO4VTVaS5atM5HvnBlHh2S4+mUmsjy23O5d0YOnVITmT9tCJcP8U0MfnLLASYO6e4JYQ6WoT9jyWZW7zxihTONdo8pGyMk/g/H+S/vYMbiTR4F5P5evD6/Wfp7/UfjyO6axmXnd6Os+jS7DvuauAoKg69xc6Cwgs4pCSy/7WIeet138bN5z//DZ39Yzwz2LLyS7C7JjfIluQIJ+rF65xGPwnXn/YBr8beNe75m1ijfFTtnj+3rMYl1SI5n3MCuPhn6lw7qFjQh1Pw1RnvDlI0RFPe6Lt7lS5ZtPUhGUjwZSXEeB/iOL0t4c9dRUuNjGJIZGCQwtEdakKsH5+olW1BV7rxycNDjIi6fTDC+KCxnxINvsfPwSTqnJpDVOZlrL+rFZ0dLuW54b/p0SmZIZjqlVTX8vxW7qappnCNpwSs7uf3y/ojA4vV7A4IAOqYk8MzckQG5QjOWbPaU7SmpqGFj/gmf4xvzTyAiFgFmtHtM2RgBuGc0i1btZv60IT7Hdh4+Sf97Xmfp5v2MG9SVf1+2nTH9OtMhJYH8466ZR3piLIIriGz3kVOBHQShQ3Icu4+WcrCwnIfXBc6UOqfGk5EUz/Uj+wQ525fCsmpyB3ThnqlD+Pu/XcxdkwdzRU4P5/oVLNt6MGh2fnysMKR7Kt3T4hFnf/v8iQzrmcFfth3iZGUtV+aErtgcNOPfa/qUkRQX4HdKT4zzhEv7YxWYjfaEKRsjAO+CjP6rXXrz0ysGcUVOD17a/iVflVTiLuZcWnWajsnxKGeetclxoaOrBndPJSU+hkdvvJD0xFje2HGEOL8pTGlFrWuJgT0Nq8794rYCJv9hA9977F0u/fU7XP3NTJ/jBcUVARHVnVPiWfbD0bzx4/F896LzqDmtXLRoHTsPn+R7I/vQt0tqyNphX3xdRofkeDb8/DKf4xt+fpknGu1kZS0nK319MScrazhZGRhpZxWYjfaGKRvDB/fDzP+hOntMVkDbJW/tDZj5uCnyc3CHs1rtO1FGUXkt/2fZdib9fiNHSquo9Qsrq6lTsjonc7CoMug1/H/IpxWOnaqhokY5VX2aax59z+f4qarTDParPF1YXsOSt/ay5K29vLnzmM8x93o1wSLHDpw4xcTfvsP0hzfxsJ//ym1yA5cSHz/YN4hg/OBuQWdE9VVgtlmP0dYwZWN4cL9Nu6sPe1Nz2ldbzM3NZm3eMRa84lvqPxT+ysP3GFTW1lGnUFQeOgrr0y9PhjzWhFQeDhWWk9Mzg7SEWOJi8CRWLt18IGCBt4Ur8ygurw665HJGUhznZ7pydp50invm9EhnTm62T2RZSUUNG/ec8Dk/WFVrCF+B2WY9RlvEkjodLKkzeGHNoT3SGd2/M39+7wu81zy7cWQffji+H//yxAcUlVVSfTq8QokU13yrB6/848hZX2dI91R2H/ONdpubm82C6Tk+BSmBoLXD6urqfEyO+x6c4lmAzdvh39DaY8H+L7zrtIU6ZsmgRrSxpE6j0QR7m847UspTW1yKxu0wv3FkH/6y7RCPbzrAn28ZSU1dyygagOWfNF7RdEsLNFv5KxqAOueevMORg9UOq6urC1gkbcbizahqgCJpaO0xdymgYPk3tu6M0RYxZWN4qK+w5qDuaXRMSSAxIZbkhDiWvX+Qy3+7gZrTLTc7duu4xjxmC8uCm+oEuHFUH09hzjV5xzwP93DhyIeKKoIuknaoqKIRo/LFvQR2sPybUOvOmJXCaM2YGc3BzGhnfDbjBnXlnc+OUVZ9OqQPZdaoPizbeijosdbE1GHdec3P2R8XI0FnYgO6pvLm/x1HTEwMxeXVjcp5+eLrMs8iaXV1dRwqqqBvl9DFSc8GW3fGaE3YejaNxJSNC3cy5/yXd/gsLuZPWmIsp6pOR3FkTSNUYc5gZHVOaTMPbFt3xmgtmM/GaBTuUFq3T+HWS30LUHZK9k08rKxp/YoGXIpGgLSEWM7PTAtrbhs3sGvUl2JuKrbujNHWMGVj8MXXZcxYvImFK/Ooq6tj/sufMmOJr8O7qOJMKLDLDBXtUTYdBa4b0YdV83KZ8a2eAcf7dEpi1qgsNuYHD0M2DOPsiZiyEZE+IvKWiOSJyE4R+ZEj7ywia0Rkj/PdyZGLiDwsIvki8omIXOR1rdlO+z0iMttLPlxEPnXOeVicV71QfRiBFJdXc9MTW8lIivdUDFi29RC1p0+TlhjLDSN6Eef3K1HVSKxn1iQSYhs2ktlj+3Kquo6PD5UwJNN38bPR/bqw8JphDTKhtYZkytYwBsNoLJGc2dQCP1XVocAY4DYRyQHuAtap6iBgnbMPMAUY5HxuBR4Fl+IA7gNGA6OA+7yUx6NOW/d5kx15qD4MP9yZ6t6LfgFMuaAHlw/uyvPbvgyYxZzWZlh9s5modiLhBncPX/Dz6S0H6JAcz7O3jKKsupZZo7LY+8BkZo3K4v39hRwqqmiQogmXTBkNJVBcXs30xZtYuHKXM4ZdTHcqcTflWqa0jGgRMWWjqodVdbuzXQrkAb2AmcDTTrOngWuc7ZnAM+riPaCjiPQErgLWqGqhqhYBa4DJzrEMVX1XXX8xz/hdK1gfhh8iwk1jAotb/v2jw6z49GgLjKh+EoNMZr4sLg/Z/sZRvVm3+zgHC8vp2yWVP/9gNBvzT/DAa5+x8JphjB/cjZue2FrvwzZcCZloZfWrKiXlNSzdfMAZwwFKymsaHfZsVQiMaBMVn42IZAMXAu8Dmap6GFwKCejuNOsFeIc/FTiycPKCIHLC9GF4UVxeTeGpSqYtDr2IWDhClfuPNFVBnqu1p+tCjuejQyXk9u/sUShZnVN8Co0ue/+gT92xUIRKpgTXv+Wkod1D1jJrLjqmJHDt8N4+smuH9250gEB9tdcMo7mJuLIRkTTgb8CPVTV0cavgeXnaBHljxnariGwTkW3HjzesmnB74Yuvy7h6yWZ+u+Zz1/rFTaCFigYEpep06PHkHS7l+W0FnodpUzPwgyVT3r9iFz976R9M/N0GKqt9I/TmTRgQkax+/0s2pQurQmBEm4gqGxGJx6Volqnq3x3xUccEhvPtzrgrALztOb2Br+qR9w4iD9eHD6r6mKqOUNUR3bp1C9akXVJcXs2/PP4+6YlxLHv/EOU1rUhrRBD3w7SpGfjuEjJzxmYzNzcbcC39/NKHX9IhOT4gL2nx+vxmz+ovLq/mrx8W+Mj++mFBo81fVoWgYZhfq/mIZDSaAE8Aear6O69DywF3RNls4FUv+c1OVNoYoMQxga0GrhSRTk5gwJXAaudYqYiMcfq62e9awfo4J6jvD6RDcjzjB3ULCApoD4T7Qf/i75+gqhSVVfHGjsNB646Fw11C5t4ZOSyYnuNzrLDM9994zti+rHXK3TQnIkJGUjxzxmaz/6GpzBmbTUZSfKNnJOFqrxkuzK/VvESsgoCIXAJsBD7lTAX4e3D5bV4EsoCDwHWqWugojCW4IsrKgTmqus251lznXIAHVPVJRz4CeApIBl4H5qmqikiXYH2EG297qSBQXF7NtIc3cWVOJvfOyOH+Fbt4c9dRVt1xCR1TEigur+bg12Xc9txHZ1W7q7USrrJBXAxsvWciNz3xAbuPlrLuJ+PJ7prW6Az8YBWZ/ZkzNps7Jg70LJzWnDRX9QCrQhCecJW3zdx4BitX00jai7IpKqti/K/fptRr9cf0pDg2/OwyRISpf9zIVyWVJMSKJ2y4LdEpOYaiCte7y7Rhmby+8yh1QI/0RC4f0o0V/zjMqepAZTO0Rzp5R0o9+8N6ZrBiXi4xMY2f3HvXJps/bQhTH97Ebr9rn6ysYcW8S+zh3cZRVfrd/Zpnf/9DU03R+GHlas5ROqYkcJ1ftNJ1w3ujqtTV1XHZ4K4AbVLRAB5FA7DKUTRpibG88K9j+PnkIXRMTfDxqbgZM6CLz763oglndgx2zLsi88nKWsqqarl2eC/2PTiFubn9KK2q5c8/GG2Kpo1jfq3mxZRNO8T/b6GiqpZLf/0O43/9Dm9/dqJlBhVBrhqWSd8uqXRMSWDZD0Zzx8SBrM07xtzcbD5aMIk5udn8+b0vfM6ZsXgzdXV1Ye3y4Y65a5N1TElgxbxL+PW13yImJsazFECkKj4b0cP8Ws2LmdEc2oMZrbi8mrq6Oi799TsByxq3Z+JjhffvnsCSt/YFXVHzwIlTTPztOwzp4TKdzVi8mc+OlrLup5eS1TnFVsQ0QmJ+rfoxn00jaevKxrMWzcCuvLPnOBlJ8ZysqKaguLKlh3ZW3DiyNz8c35/Lf7uhQe3n5vZj3oQBnpmH+wFRUlETcr2ZYHb5kooaT4JjMJu9PYSMtk5z/YbNZ9OO8fYjFJVVUVRWRYfkeCYO6c6yrQcpKKpg1+GTpCW2/Wzw7V8UcejrUw1uP2/CAGY+siXA9NUhOd7jo4mJiaFDcjyqiqpy/4pdPteY/8oOZizexP0rdrFwpe+xhSvzKCqrspBYo03TEmHdcfU3MVoT3pFQN1+cxYzFm0HgnTsvpbTK15a851hpiKu0DWIEdh8rY/ZT20mOj6Gipv51DRavz2fiEFfZGLf5a25uP58yLN7/hvMmDOClDws8EXuL1+9lbd5Rxg/qxpNbDnjOmTM2GxF4c9cRbr+8v6fUS6g+DKM1412uKFq/YZvZtDE6JMczql8nlm7ez2W/cflmSitruWjROl768Euftm004MyDu/zM0B7pfOfCXj7HhmSm8eEvJpCWGEt6UhwfLZjkOHCPccfEgT5tF0wfSknFmWKVGUlxjBvYlaWb93PhwrWUVtVynVNfzO3gX/SdC3yuce+MHOZNGIgqLHlrH/OnDQnow/w4RluhJcoVmbJpYxwsLOfv27+sv2E74pm5I1j+j8M+si+LKzlZWcuyW0bxzp2X0jElgdsv788zc0eyeP1en7bzX97hMRkUlVWxaFUeG/N9o/IWTM9BRBAROiTHBw157ZAcz1XDengKePofN/+n0VZoibBuUzZtjKJTla2qAGY0uOIPm6ioruWfLjqPPp2SyemRTnpSHNMXb+Ka/36X36z+nIUrd3Hpb97hxv99n9U7j/iEq27Yc5xxg87MZJZuPkB6oq8F2fsPLVTI68nK2oC3QXdujYXEGm2Jlgjrtmg0h7YQjVZcXs0nBwu5+akPW3ooUWfcwC4snT2cB1//3MeX4s+csX25Y+Ig12qiTh6M2+l54cK1Qdq7fDFr8475rNQZLFLHPePxDoWeMzabeRMGEBMTY9FoRpsi2tFoFiDQyvm0oJicnumUVp1m+sMbOS/j3Hygbcz/mulLtjC8b30rfLtszpf+5h1Q2PDzy1i8Pj+gUrIbt5163gRfP4/3H5230lq98wjpSXFOVQZ46cMC1uQdZeW8S87q/gwj2gT7jUcSm9k4tMaZzacFxcxYspnOqQlsvftycu57s82WmWlp0hPjuHZ4L0B8ZkbuGmmLVu1mbd5Rnr1lFFmdU0K+7RWVVbF4fT5LN5+5hiV6GucylmfTDkhNiCFGXOXrB85fbYomBKkJMaQlxhIfG/phv+Hnl3HHxEGs232MOWOz2T5/IsN6ZrDz8En63/M6SzfvZ9zArtz0xNawuQedUhMDlhcwRWMY9WPKppVSXF7N7Ce3MaBrSksPpdXTu1My1w7vTU0YZbx4/V6f9Wg6pyXx7C0jfdr8YFy2J0cn1FLJVpzRMJqGKZtWhrs6QIfkeC4f3JU9x8tbekhNIqEZfln+c4WEWCEuyHXLq+v4/thsH1laYizpid75N0c5WFjuUQqFpyoZ/+u3fc6Z8LsNVNWGX9rZijMaRtMwn41DS/ts3Epm5iNbmDikO3dMHMh9r+5k+SeH6z+5BTkvI4GvTjZ/iYvzu6dRWlkdcO3vj+3LU1t8KzjfOKoPSfGxPn6UWaP78NMrBtM5LQlV5WBhObMef5+S8hquHd6bqtpantvqGzQQHysBs6M5Y7O5d0aOj8KxumiGcQbz2bQh3OVT3KVWntxygAsXrm31igZodkUztGc66YlxFFdUkzuoe8DxSr9q1sN6ZvDO5yd4c5fvbGPjnq89tdBEhKzOKVyZk0lpVS1PbjkQoGgA3vzxOJ/9OWOzWbc7cGlnd5FP97VN0RhG/djMxqElZzaqyvxXdrDs/YMBx5LihMratvt/dMPI3jz/QeCDPTMtnmOnagh2Z0N6pFFSXkNMjJCeGM/uo2dqvMXFComxwsafX86St/axZtcR/vyD0XRIjveZbRwsLA+IKuuQHO9Twdkfd8CAmzm52cy+uC/ZXdPO6t/AMNozLT6zEZGlInJMRHZ4yTqLyBoR2eN8d3LkIiIPi0i+iHwiIhd5nTPbab9HRGZ7yYeLyKfOOQ+L86oZqo/WTElFDRs+Px70WFJ8bJRH07y8EETRAEz95nl896Lzgh7bfeQUU75xHst+MJrymlqG9czwHKs9rVzz7V5OVNhQVsy7xLNwmnu2UVJRExBVNmPxJua/8mlAX+dnprHvwSnMGp3F7qOlpCXEelb6fOnDAmY9vtWqORtGMxBJM9pTwGQ/2V3AOlUdBKxz9gGmAIOcz63Ao+BSHMB9wGhgFHCfl/J41GnrPm9yPX20WjKS4shICl5ttbii7S6CJhB05gLw4rYCNn5+gvTEOB9l4mb+tCFkd01j+e2XsGJers+xRd/5hqeOWTATlndFW3dU2fhB3Xhr93EEyOmRzpzcbOJiYM/RUxwqqmDRNRew7ifjuW5EH57ccoClmw9QWlnLVcN6WDVnw2gGIqZsVHUDUOgnngk87Ww/DVzjJX9GXbwHdBSRnsBVwBpVLVTVImANMNk5lqGq76rLDviM37WC9dFqOVlZ26ZX1kxLjKVfp6QAeWpiLH+a9W0f2Y2j+nDDiN4oSmyMcNWwTB/TlZsFr+z0ROUtWrXb51h9ocbBKtou+s4FrLrjEm4Y2YddR0p5cvMBauvghlFZHnNbdtc07p1hOTSGEQmiHSCQqaqHAZxvtwe4F3DIq12BIwsnLwgiD9dHq6S4vNpT8r6tkRjjiuDKSIonPj6w8tGMb/bkZ3/b4SN7bushnt9WQHl1HZMv6El6ku95ex+YzKxRWWzMP+FZYbOxocahcmE6piTwwHe/4SNf9J0LPMrEcmgMI3K0lmi0YK+O2gR54zoVuVVEtonItuPHg/tMIskXX5dx9ZLNLHh1J+98fowhmemc1yGJ1PgY0pPi2D5/Iv904XkktVK3TVUd1JxWviqp5PNjp4jx+195/oMCtE6ZMzab/Q9NZY5fLsyTWw7wpF8Y8wOvfcbCa4Z5imK6EzHdMwz3ejPhIsBCKaji8uqQyqS4vJri8mrnvGyf/BzLoTGMsyfayuaoYwLD+T7myAuAPl7tegNf1SPvHUQero8AVPUxVR2hqiO6devW5JtqCsXl1dz4v++RECcse/8gBcWV7D5aSlF5NVd/uyfv3Hkpv3nzc7bs/ZrY2FaqbfxISYjl+2P7+shW3nGJxzQVyhp15uGeHfTh3thQ41AKSkSCKqGDheVO6PleXr1tLKow85EtzJswoF7FZhhGw4i2slkOuCPKZgOveslvdqLSxgAljglsNXCliHRyAgOuBFY7x0pFZIwThXaz37WC9dGqcL1N15B/rMxHXlFTx/MffMlFi9bx3NZDHDtZRaVfVntzExsjdEw5+wLgdafreH+fr5vufze6yvG7ZhvHPJFe3lTWnObqJZtRhVdvG8vi9XvPej30YAoqlBLK6pziCSi4cOFantxygElDMz3nGIZx9kQsz0ZEngcuA7oCR3FFlb0CvAhkAQeB61S10FEYS3BFlJUDc1R1m3OducA9zmUfUNUnHfkIXBFvycDrwDxVVRHpEqyP+sYb7TwbVeXuv33MC9u+qr9xFEiOhYowOu38zDSG9+0YNBnSjX8G/rCeGZRW1XpmB95VEiYN7c68CQNZvH4va/OOMm5gV5ZtPZNnFO1Kyqrqk4Oz/6GpFhhgGA2goXk2ltTp0BLK5s6/fszftrcOZVMfg7ulsu/rcq4b3ot3933Nga8rPMc6pcRTXVtHh+R4viqp9Mj3PTiFk5W1AbODUAuTtdTD3h0Y4L0omi0bYBgNo8WTOo3wFJdX88aOIy09jLB0Tj2jJD4/XsY1F57Hz646n9N1vu2KymuY+a2eXDbY1++1aNXuoDkq/iYu9wqY3kQzCsyKaxpG5DFlEyXcJiRwvUmfrKwlLTF6C6V2T4unT5BcmGDsfWAys0ZnkZboG5jw62u/RafURMYPDgymuPOq89m09+smPbBb+mHflIg3wzAah5nRHCJpRnMX2pw0NJMF04eycGUea/OOsuSGb3P9Y+9RXnNmqiDA4Mw0Pjt6Kuw1szom8VVJJaHKpsUA3hOQ1IRYqmpPU+sljBUItgTMrNF9uP/qYdz76q4AP8q8CQOY+cgW0hPjfJIxZ43K4s6rBntmLY2thmyVlA2jbWJmtBbEfxajqgHlUyYNzaR3p2RPZWI3KQmxPPeDUWGvHwMcLPZVNMlxQmpCLP27ppASL9Thyux/+85LmZubjQg+igbOKJoYgRtG9uL7F/clNTGWt3Yf51BRBRvzTzBnrG/OiYjw7C2jOFlZw9zcfq66Yk4SpruEDDS+GrJVUjaM9o3NbByaa2YTahbz6m1juXDhWk+7/Q9NpaSihmkPb+LKnEzunZHD/St2sXrnES4f0t2nAnRcjGvhsB4ZSRwurgiIGouPEd74US7zXviEXV6zjbm52cybMJAOyfEcLCznst+8E3TMH/5iAp3TXCa2L74uY9bjW7lqWA/mTRjA4vX5rM07xqu3jfVRAjYTMQwDLBqt0TSXsgke2ZSNqitj/ozMFe3kjsRyP7R3fFnCvy/bzvjB3bj/6hz+4++fsnnPCU5W1lJW3bh8m48WTPKEGVdWn+a5Dw4FbTesZwYr5uUSExNjkVmGYTQKM6O1EMGKQM6bMJB1u48FdYD7l8a/7bmPGNO/C+98doyrl2xh6/5Cxg7oQkwTHvTuxdiWbj4QVNHcMLI3w3pm8NnRUg4VVYQcvykawzDOluiFQ50jBCvm6C6D4lYsC6YP5Y6JAwPMTt6l8QEKil05KwcLK8jpkc6uI6XUh3uWsmjVbo/5zntG5c2D3/0mqsqhogr6dkkNOf6FK/NM4RiGcVbYzKaZCRXG2xDnebBZhZuDReU++/Gxwls/HU9aYixpiXF8tGASs0ZlcbKyhpOVtSyYPtRT+iUUC1fmISIeRRNu/JZzYhjG2WA+G4em+GxCOcmb6jwP5i9xk5YYx5+RXAYAAAkVSURBVHUjenPv9Bzmv/wpb39+nFV3jPNEvXVKTaSorMqz7V4a+aYntjJpaCbzJgxg/K/eBoENP7vMUyYmWD6Je/wlFTVkJMVxsrLWs29BAIZheGMBAo2kscomVNSZ98O7sUrHfc3R/Trz3r6vyUiKp7SqhlH9OvPu3kJWzsv1KBL/a4Uaz7O3jPIsDuavjMKNpyH3ZxiG0VBlYz6bJuLtX3HPRObm9vOUZ2nKw7pjSgLP3jKKf3n8fcYP6sb9M3O499VdbNhznOd+OJpOqYlAcDNcqPG4FQ3gOT/UNRpzf4ZhGI3BfDZNpL6oLe+HtXciZ30P66zOKVyR04NlWw8y4BdvsGzrQa7I6UFW55SzGk9jsag0wzCaE1M2TaS+JYSb+rBu6nnNvaSxLZFsGEZzYsqmidQXtdXUh3VTz2vuKDKLSjMMozmxAAGH5oxGcx9rioP9bBzzzV1CxkrSGIZRHxaN1kgiUfW5qQ9re8gbhtFWsGi0VoC3gmhMJeOmnmcYhtFaMZ+NYRiGEXHarbIRkcki8pmI5IvIXS09HsMwjHOZdqlsRCQWeASYAuQAN4hITsuOyjAM49ylXSobYBSQr6r7VLUaeAGY2cJjMgzDOGdpr8qmF+C9gEuBI/NBRG4VkW0isu348eNRG5xhGMa5RnuNRguWbh8Q462qjwGPAYjIcRH5ItIDizJdgRMtPYgocC7cp91j+6G93WffhjRqr8qmAOjjtd8b+CrcCaraLaIjagFEZFtD4t/bOufCfdo9th/Olfv0p72a0T4ABolIPxFJAK4HlrfwmAzDMM5Z2uXMRlVrReR2YDUQCyxV1Z0tPCzDMIxzlnapbABU9TXgtZYeRwvzWEsPIEqcC/dp99h+OFfu0werjWYYhmFEnPbqszEMwzBaEaZs2hgislREjonIDi9ZZxFZIyJ7nO9OjlxE5GGnZM8nInKR1zmznfZ7RGR2S9xLKESkj4i8JSJ5IrJTRH7kyNvNfYpIkohsFZF/OPf4n468n4i874z3L06ACyKS6OznO8ezva51tyP/TESuapk7Co2IxIrIRyKy0tlvj/d4QEQ+FZGPRWSbI2s3v9dmQVXt04Y+wHjgImCHl+xXwF3O9l3AfznbU4HXceUdjQHed+SdgX3Odydnu1NL35vX/fQELnK204HPcZUdajf36Yw1zdmOB953xv4icL0j/xPwb872vwN/cravB/7ibOcA/wASgX7AXiC2pe/P715/AjwHrHT22+M9HgC6+snaze+1OT42s2ljqOoGoNBPPBN42tl+GrjGS/6MungP6CgiPYGrgDWqWqiqRcAaYHLkR98wVPWwqm53tkuBPFwVINrNfTpjPeXsxjsfBSYALzly/3t03/tLwEQREUf+gqpWqep+IB9XuaZWgYj0BqYBjzv7Qju7xzC0m99rc2DKpn2QqaqHwfWgBro78lBlexpUzqc14JhSLsT15t+u7tMxL30MHMP1YNkLFKtqrdPEe7yee3GOlwBdaOX3CPwB+DlQ5+x3of3dI7heFN4UkQ9F5FZH1q5+r2dLuw19NoDQZXsaVM6npRGRNOBvwI9V9aTrJTd40yCyVn+fqnoa+LaIdAReBoYGa+Z8t7l7FJHpwDFV/VBELnOLgzRts/foRa6qfiUi3YE1IrI7TNu2fJ9NxmY27YOjzjQc5/uYIw9VtqfR5XyijYjE41I0y1T174643d0ngKoWA2/jst93FBH3S6D3eD334hzvgMuc2prvMRe4WkQO4Kq8PgHXTKc93SMAqvqV830M14vDKNrp77WpmLJpHywH3JErs4FXveQ3O9EvY4ASZzq/GrhSRDo5ETJXOrJWgWOnfwLIU9XfeR1qN/cpIt2cGQ0ikgxMwuWbegu41mnmf4/ue78WWK8ur/Jy4HonkqsfMAjYGp27CI+q3q2qvVU1G5fDf72qzqId3SOAiKSKSLp7G9fvbAft6PfaLLR0hIJ9GvcBngcOAzW43oRuwWXXXgfscb47O20F1yJye4FPgRFe15mLy9GaD8xp6fvyu8dLcJkPPgE+dj5T29N9At8EPnLucQdwryPvj+tBmg/8FUh05EnOfr5zvL/XtX7h3PtnwJSWvrcQ93sZZ6LR2tU9OvfzD+ezE/iFI283v9fm+FgFAcMwDCPimBnNMAzDiDimbAzDMIyIY8rGMAzDiDimbAzDMIyIY8rGMAzDiDimbAyjGRCRTBF5TkT2OSVL3hWR7wRply1eFbu95PeLyKQG9HOhiGhrrHxsGOEwZWMYZ4mThPoKsEFV+6vqcFxJjL392oUsD6Wq96rq2gZ0dwOwyfkOOhYRsb9ro9VhP0rDOHsmANWq+ie3QFW/UNXFIvJ9EfmriKwA3gx1ARF5SkSuFZEpIvKil/wy51y3UrsW+D6uTPMkR54trrV//hvYDvQRkSud2dV2p/80p+29IvKBiOwQkcckTME5w2hOTNkYxtkzDNdDPhQXA7NVdUIDrrUGGOOUPQH4HvAXZzsX2K+qe3HVUpvqdd75uMrWXwiUAfOBSap6EbAN15oyAEtUdaSqXgAkA9MbMCbDOGtM2RhGMyMij4hrBc4PHNEaVfVfgygo6iqt/wYwwzG7TeNMTa0bcBW0xPn2NqV9oa61UcBV0DMH2OwsYTAb6Oscu1xcq2B+imtGNqzxd2gYjceWGDCMs2cn8E/uHVW9Tf5/e3eoEkEUhXH8+7DIVq2LBlHfwWDZYpENWuwG8QEMFp/C4gMsBtFgFqO4SV/AoE2bWY7hjOw6jEGG0/4/GAYG7p1pH+fe4Vx7WVlRSFlp/MelpGNlx+NpRHzaXmjesWv7VNlfa+mnAWTrHVYG3K99nWbZ7VzZi+vV9pmyHxlQjsoG6O9O0qLto7lngx7z3SuP/j7UbAltJOkpIoYRsRoRK8ojGMYd4x8kbdlekyTbA9vrmgXLR7OHs9cxFihB2AA9RXazHUvatv1i+1F5DPDJH0M2bL/NXfut+b4k3Uraae5SLpldt+a5knTQ8T3vyp8IJrafleGzGXluzoWy0/CNpGl7LFCFrs8AgHJUNgCAcoQNAKAcYQMAKEfYAADKETYAgHKEDQCgHGEDAChH2AAAyn0DwhWsdRRFDD8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot('GrLivArea', 'SalePrice', kind = 'scatter', marker = 'x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### In building regressions below, a subset of our data will be used.\n",
    "\n",
    "### Practice subsetting a DataFrame below.\n",
    "### Create a DataFrame only containing the \"Street\" and \"Alley\" columns from \n",
    "### the `data` DataFrame.\n",
    "\n",
    "### Assign to 'ans1'\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 2",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"code\"></a>\n",
    "### Coding Ridge Regression   \n",
    "\n",
    "#### Preprocessing\n",
    "Before implementing Ridge Regression, it is important to mean-center our target variable and mean-center and standardize observations. We will do this according to the following:  \n",
    "#### Mean Center Target\n",
    "$$y_{cent} = y_0 - \\bar{y}$$\n",
    "\n",
    "#### Standardize Observations\n",
    "$$X_{std} = \\frac{X_0-\\bar{X}}{s_{X}}$$\n",
    "\n",
    "Where $\\bar{X}$ is the sample mean of X and $s_{X}$ is the sample standard deviation of X.  \n",
    "\n",
    "NOTE: The sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"\n",
    "\n",
    "#### Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Why are the centering / standardization transformations described above important for ridge regression?\n",
    "### 'a') Regression works best when values are unitless\n",
    "### 'b') The transformations makes the regression more interpretable\n",
    "### 'c') Ridge penalizes large coefficients; the transformations make the coefficients of similar scales\n",
    "### 'd') It isn't important\n",
    "\n",
    "### Assign character associated with your choice as a string to ans1\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 3",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 4:\n",
    "$$X_{std} = \\frac{X_0-\\bar{X}}{s_{X}}$$  \n",
    "\n",
    "NOTE: The sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\".  \n",
    "If your answer does not match the example answer, check the default degrees of freedom in your standard deviation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called \"standardize\" \n",
    "### ACCEPT one input, a list of numbers\n",
    "### RETURN a list where those values have been standardized.\n",
    "\n",
    "### To standardize, subtract the mean of the list and divide by standard deviation.\n",
    "### Please use np.std for calculating standard deviation\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def standardize( num_list):\n",
    "    \"\"\"\n",
    "    Standardize the given list of numbers\n",
    "    \n",
    "    Positional arguments:\n",
    "        num_list -- a list of numbers\n",
    "    \n",
    "    Example:\n",
    "        num_list = [1,2,3,3,4,4,5,5,5,5,5]\n",
    "        nl_std = standardize(num_list)\n",
    "        print(np.round(nl_std,2))\n",
    "        #--> np.array([-2.11, -1.36, -0.61, -0.61,  \n",
    "                0.14,  0.14,  0.88,  0.88,  0.88,\n",
    "                0.88,  0.88])\n",
    "    \n",
    "    NOTE: the sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"\n",
    "    \"\"\"\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 4",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Below we will create a function which will preprocess  our data by performing:\n",
    "* mean subtraction from $y$,\n",
    "* dimension standardization for $x$.\n",
    "\n",
    "Both according to the equations set out below.\n",
    "\n",
    "#### Mean Center Target\n",
    "$$y_{cent} = y_0 - \\bar{y}$$\n",
    "\n",
    "#### Standardize Observations\n",
    "$$X_{std} = \\frac{X_0-\\bar{X}}{s_{X}}$$  \n",
    "\n",
    "NOTE: the sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"  \n",
    "If your answer does not match the example answer, check the default degrees of freedom in your standard deviation function.\n",
    "\n",
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_regularization(data, y_column_name, x_column_names):\n",
    "        \n",
    "    names = x_column_names.copy()\n",
    "    names.append(y_column_name)\n",
    "    subs = data[names].copy()\n",
    "\n",
    "    \n",
    "    y_mean = np.mean(data[y_column_name])\n",
    "    subs[y_column_name] = data[y_column_name].apply(lambda x_data: x_data- y_mean)\n",
    "\n",
    "    for col_names in x_column_names:\n",
    "        mean_x = np.mean(data[col_names])\n",
    "        std_x = np.std(data[col_names])\n",
    "        subs[col_names] = data[col_names].apply(lambda x_data: (x_data-mean_x)/std_x)\n",
    "    \n",
    "    return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.370333</td>\n",
       "      <td>1.050994</td>\n",
       "      <td>-0.207142</td>\n",
       "      <td>27578.80411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.482512</td>\n",
       "      <td>0.156734</td>\n",
       "      <td>-0.091886</td>\n",
       "      <td>578.80411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.515013</td>\n",
       "      <td>0.984752</td>\n",
       "      <td>0.073480</td>\n",
       "      <td>42578.80411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.383659</td>\n",
       "      <td>-1.863632</td>\n",
       "      <td>-0.096897</td>\n",
       "      <td>-40921.19589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.299326</td>\n",
       "      <td>0.951632</td>\n",
       "      <td>0.375148</td>\n",
       "      <td>69078.80411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GrLivArea  YearBuilt   LotArea    SalePrice\n",
       "0   0.370333   1.050994 -0.207142  27578.80411\n",
       "1  -0.482512   0.156734 -0.091886    578.80411\n",
       "2   0.515013   0.984752  0.073480  42578.80411\n",
       "3   0.383659  -1.863632 -0.096897 -40921.19589\n",
       "4   1.299326   0.951632  0.375148  69078.80411"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    prepro_data = preprocess_for_regularization(data, 'SalePrice', ['GrLivArea','YearBuilt', 'LotArea'])\n",
    "    prepro_data.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Perform mean subtraction and dimension standardization on data\\n     \\n Positional argument:\\n     data -- a pandas dataframe of the data to pre-process\\n     y_column_name -- the name (string) of the column that contains\\n         the target of the training data.\\n     x_column_names -- a *list* of the names of columns that contain the\\n         observations to be standardized\\n     \\n Returns:\\n     Return a DataFrame consisting only of the columns included\\n     in `y_column_name` and `x_column_names`.\\n     Where the y_column has been mean-centered, and the\\n     x_columns have been mean-centered/standardized.\\n     \\n     \\n Example:\\n     data = pd.read_csv(tr_path).head()\\n     prepro_data = preprocess_for_regularization(data,\\'SalePrice\\', [\\'GrLivArea\\',\\'YearBuilt\\'])\\n     \\n     print(prepro_data) #-->\\n                GrLivArea  YearBuilt  SalePrice\\n             0  -0.082772   0.716753     7800.0\\n             1  -1.590161  -0.089594   -19200.0\\n             2   0.172946   0.657024    22800.0\\n             3  -0.059219  -1.911342   -60700.0\\n             4   1.559205   0.627159    49300.0\\n \\n NOTE: The sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"\\n \\n If your answer does not match the example answer,\\n check the default degrees of freedom in your standard deviation function.\\n '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   \"\"\"\n",
    "    Perform mean subtraction and dimension standardization on data\n",
    "        \n",
    "    Positional argument:\n",
    "        data -- a pandas dataframe of the data to pre-process\n",
    "        y_column_name -- the name (string) of the column that contains\n",
    "            the target of the training data.\n",
    "        x_column_names -- a *list* of the names of columns that contain the\n",
    "            observations to be standardized\n",
    "        \n",
    "    Returns:\n",
    "        Return a DataFrame consisting only of the columns included\n",
    "        in `y_column_name` and `x_column_names`.\n",
    "        Where the y_column has been mean-centered, and the\n",
    "        x_columns have been mean-centered/standardized.\n",
    "        \n",
    "        \n",
    "    Example:\n",
    "        data = pd.read_csv(tr_path).head()\n",
    "        prepro_data = preprocess_for_regularization(data,'SalePrice', ['GrLivArea','YearBuilt'])\n",
    "        \n",
    "        print(prepro_data) #-->\n",
    "                   GrLivArea  YearBuilt  SalePrice\n",
    "                0  -0.082772   0.716753     7800.0\n",
    "                1  -1.590161  -0.089594   -19200.0\n",
    "                2   0.172946   0.657024    22800.0\n",
    "                3  -0.059219  -1.911342   -60700.0\n",
    "                4   1.559205   0.627159    49300.0\n",
    "    \n",
    "    NOTE: The sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"\n",
    "    \n",
    "    If your answer does not match the example answer,\n",
    "    check the default degrees of freedom in your standard deviation function.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GrLivArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.370333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.482512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.515013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.383659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.299326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GrLivArea\n",
       "0   0.370333\n",
       "1  -0.482512\n",
       "2   0.515013\n",
       "3   0.383659\n",
       "4   1.299326"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    x_data = data['GrLivArea'].values\n",
    "    x_mean = np.mean(x_data)\n",
    "    x_std = np.std(x_data)\n",
    "    \n",
    "    for top in x_data:\n",
    "        top = x_data - x_mean\n",
    "    \n",
    "    std_obs = top/x_std\n",
    "    test = pd.DataFrame(data=std_obs, columns = ['GrLivArea'])\n",
    "    test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 5",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Next, you'll implement the equation for ridge regression using the closed form equation:  \n",
    "\n",
    "$$w_{RR}=(\\lambda+X^TX)^{-1}X^Ty$$  \n",
    "\n",
    "The function will be very similar to the function you wrote for Least Squares Regression with a slightly different matrix to invert.  \n",
    "\n",
    "NB: Many `numpy` matrix functions will be useful. e.g. `np.matmul`, `np.linalg.inv`, `np.ones`, `np.transpose`, and`np.identity`.\n",
    "\n",
    "The main change from Least Squares Regression is that $\\lambda$ is a parameter *we* must set. This is different from the $w$ parameters that we calculate from either closed form or approximation algorithms.  \n",
    "\n",
    "We will address tuning parameters such as $\\lambda$ in the next section.  \n",
    "\n",
    "#### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called \"ridge_regression_weights\"\n",
    "### ACCEPT three inputs:\n",
    "### Two matricies corresponding to the x inputs and y target\n",
    "### and a number (int or float) for the lambda parameter\n",
    "\n",
    "### RETURN a numpy array of regression weights\n",
    "\n",
    "### The following must be accomplished:\n",
    "\n",
    "### Ensure the number of rows of each the X matrix is greater than the number of columns.\n",
    "### ### If not, transpose the matrix.\n",
    "### Ultimately, the y input will have length n.\n",
    "### Thus the x input should be in the shape n-by-p\n",
    "\n",
    "### *Prepend* an n-by-1 column of ones to the input_x matrix\n",
    "\n",
    "### Use the above equation to calculate the least squares weights.\n",
    "### This will involve creating the lambda matrix---\n",
    "### ### a p+1-by-p+1 matrix with the \"lambda_param\" on the diagonal\n",
    "### ### p+1-by-p+1 because of the prepended \"ones\".\n",
    "\n",
    "### NB: Pay close attention to the expected format of the returned\n",
    "### weights. It is different / simplified from Assignment 1.\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def ridge_regression_weights(input_x, output_y, lambda_param):\n",
    "    \n",
    "    \n",
    "    weights = np.array([])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "     my = np.array([208500, 181500, 223500, \n",
    "                                140000, 250000, 143000, \n",
    "                                307000, 200000, 129900, \n",
    "                                118000])\n",
    "     mx = np.array([[1710, 1262, 1786, \n",
    "                                1717, 2198, 1362, \n",
    "                                1694, 2090, 1774, \n",
    "                                1077], \n",
    "                               [2003, 1976, 2001, \n",
    "                                1915, 2000, 1993, \n",
    "                                2004, 1973, 1931, \n",
    "                                1939]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Ensure the number of rows of each X matrix is greater than the number of columns.\n",
    "    ### ### If not, transpose the matricies.\n",
    "    ### In particular, the y input should end up as a n-by-1 matrix, and the x input as a n-by-p matrix\n",
    "    \n",
    "    # x matrix nxp \n",
    "    mxtrs = np.transpose(mx) # (10,2)\n",
    "    mxtrs.shape\n",
    "      \n",
    "    ### *Prepend* an n-by-1 column of ones to the input_x matrix\n",
    "    prepend = np.ones(len(mxtrs)).reshape(len(mxtrs),1)\n",
    "    x_dmb = np.concatenate((prepend, mxtrs), axis = 1)\n",
    "    \n",
    "    ### This will involve creating the lambda matrix---\n",
    "    ### ### a 3-by-3  (p+1 by p+1 ) matrix with the \"lambda_param\" on the diagonal\n",
    "    lamb = np.identity(len(x_dmb[0]))\n",
    "    \n",
    "    # 2nd var: x-matrix-tranposed * y \n",
    "    sec_var = np.matmul(my,x_dmb)\n",
    "    \n",
    "    # 1st var \n",
    "    #test = np.matmul(mx,mxtrs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate ridge regression least squares weights.\n",
    "    \n",
    "    Positional arguments:\n",
    "        input_x -- 2-d matrix of input data\n",
    "        output_y -- 1-d numpy array of target values\n",
    "        lambda_param -- lambda parameter that controls how heavily\n",
    "            to penalize large weight values\n",
    "        \n",
    "    Example:\n",
    "        training_y = np.array([208500, 181500, 223500, \n",
    "                                140000, 250000, 143000, \n",
    "                                307000, 200000, 129900, \n",
    "                                118000])\n",
    "                                \n",
    "        training_x = np.array([[1710, 1262, 1786, \n",
    "                                1717, 2198, 1362, \n",
    "                                1694, 2090, 1774, \n",
    "                                1077], \n",
    "                               [2003, 1976, 2001, \n",
    "                                1915, 2000, 1993, \n",
    "                                2004, 1973, 1931, \n",
    "                                1939]])\n",
    "        lambda_param = 10\n",
    "        \n",
    "        rrw = ridge_regression_weights(training_x, training_y, lambda_param)\n",
    "        \n",
    "        print(rrw) #--> np.array([-576.67947107,   77.45913349,   31.50189177])\n",
    "        print(rrw[2]) #--> 31.50189177\n",
    "        \n",
    "    Assumptions:\n",
    "        -- output_y is a vector whose length is the same as the\n",
    "        number of observations in input_x\n",
    "        -- lambda_param has a value greater than 0\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 6",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Selecting the $\\lambda$ parameter\n",
    "\n",
    "For our final function before looking at the `sklearn` implementation of ridge regression, we will create a hyper-parameter tuning algorithm.  \n",
    "\n",
    "In ridge regression, we must pick a value for $\\lambda$. We have some intuition about $\\lambda$ from the equations that define it: small values tend to emulate the results from Least Squares, while large values will reduce the dimensionality of the problem. But the choice of $\\lambda$ can motivated with a more precise quantitative treatment.\n",
    "\n",
    "Eventually, we will look to choose the value of $\\lambda$ that minimizes validation error, which we will determine using $k$-fold cross-validation.\n",
    "\n",
    "For this example here, we will solve a simpler problem: Find a value that minimizes the of the list returned by a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Example of hiden function below:\n",
    "\n",
    "### `hidden` takes a single number as a parameter (int or float) and returns a list of 1000 numbers\n",
    "### the input must be between 0 and 50 exclusive\n",
    "\n",
    "def hidden(hp):\n",
    "    if (hp<=0) or (hp >= 50):\n",
    "        print(\"input out of bounds\")\n",
    "    \n",
    "    nums = np.logspace(0,5,num = 1000)\n",
    "    vals = nums** 43.123985172351235134687934\n",
    "    \n",
    "    user_vals = nums** hp\n",
    "    \n",
    "    return vals-user_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    u = hidden(1)\n",
    "    u.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Run the above cell and test out the functionality of `hidden`. Remember it takes a single number, between 0 and 50, as an argument\n",
    "\n",
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called \"minimize\"\n",
    "### ACCEPT one input: a function.\n",
    "\n",
    "### That function will be similar to `hidden` created above and available for your exploration.\n",
    "### Like 'hidden', the passed function will take a single argument, a number between 0 and 50 exclusive \n",
    "### and then, the function will return a numpy array of 1000 numbers.\n",
    "\n",
    "### RETURN the value that makes the mean of the array returned by 'passed_func' as close to 0 as possible\n",
    "\n",
    "### Note, you will almost certainly NOT be able to find the number that makes the mean exactly 0\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def minimize( passed_func):\n",
    "    \n",
    "    \n",
    "    \n",
    "    return float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input out of bounds\n",
      "input out of bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stellarhardwood\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in power\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-f8560002c59f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mminimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mstarting_guess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'bfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[0;32m    961\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[0mgrad_calls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmyfprime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrap_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfprime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m     \u001b[0mgfk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyfprime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mapprox_fprime\u001b[1;34m(xk, f, epsilon, *args)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m     \"\"\"\n\u001b[1;32m--> 723\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_approx_fprime_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_approx_fprime_helper\u001b[1;34m(xk, f, epsilon, args, f0)\u001b[0m\n\u001b[0;32m    661\u001b[0m         \u001b[0mei\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mei\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m         \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    664\u001b[0m         \u001b[0mei\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "    mylist = []\n",
    "    for x in range(1,50):\n",
    "        mylist.append(x)\n",
    "    \n",
    "            \n",
    "        \n",
    "    from scipy.optimize import minimize\n",
    "    starting_guess = 1\n",
    "    minimize(hidden, x0=[(1000,)])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for hp in mylist:\n",
    "        trymean = hidden(hp)\n",
    "        opt = np.mean(trymean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Find the numeric value that makes the mean of the\n",
    "    output array returned from 'passed_func' as close to 0 as possible.\n",
    "    \n",
    "    Positional Argument:\n",
    "        passed_func -- a function that takes a single number (between 0 and 50 exclusive)\n",
    "            as input, and returns a list of 1000 floats.\n",
    "        \n",
    "    Example:\n",
    "        passed_func = hidden\n",
    "        min_hidden = minimize(passed_func)\n",
    "        print(round(min_hidden,4))\n",
    "        #--> 43.1204 (answers will vary slightly, must be close to 43.123985172351)\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 7",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The above simulates hyper parameter tuning.  \n",
    "\n",
    "In the case of ridge regression, you would be searching lambda parameters to minimize validation error.  \n",
    "\n",
    "The `hidden` function would be analogous to the model building; the returned list analogous to residuals; and the mean of that list analogous to validation error.  \n",
    "\n",
    "See below for an example of using the functions built above that automatically performs hyper-parameter tuning using mean-absolute-deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lambda_search_func(lambda_param):\n",
    "    \n",
    "    # Define X and y\n",
    "    # with preprocessing\n",
    "    df = preprocess_for_regularization(data.head(50),'SalePrice', ['GrLivArea','YearBuilt'])\n",
    "    \n",
    "    y_true = df['SalePrice'].values\n",
    "    X = df[['GrLivArea','YearBuilt']].values\n",
    "    \n",
    "    # Calculate Weights then use for predictions\n",
    "    weights = ridge_regression_weights(X, y_true, lambda_param )\n",
    "    y_pred = weights[0] + np.matmul(X,weights[1:])\n",
    "    \n",
    "    # Calculate Residuals\n",
    "    resid = y_true - y_pred\n",
    "    \n",
    "    # take absolute value to tune on mean-absolute-deviation\n",
    "    # Alternatively, could use:\n",
    "    # return resid **2-S\n",
    "    # for tuning on mean-squared-error\n",
    "    \n",
    "    return abs(resid)\n",
    "\n",
    "minimize(lambda_search_func)    # --> about 1.4957957957957957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Implementing a k-folds cross-validation strategy will come in later assignments.\n",
    "\n",
    "#### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Why is cross-validation useful?\n",
    "### 'a') to minimize the liklihood of overfitting\n",
    "### 'b') Cross-validation allows us to fit on all our data\n",
    "### 'c') cross-validation standardizes outputs\n",
    "### 'd') cross-validation is not useful\n",
    "### assing the character associated with your choice as a string to ans1\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 8",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"sklearn\"></a>\n",
    "\n",
    "### Ridge Regression in `sklearn`  \n",
    "\n",
    "Below gives the syntax for implementing ridge regression in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeastSquares Intercept: -2024976.362585636 Coefs: [  95.16733349 1045.86241944] \n",
      "\n",
      "Ridge alpha = 100000 Intercept: -1876820.7548806793 Coefs: [ 96.01060155 970.05661101] \n",
      "\n",
      "Ridge, alpha = 0 Intercept: -2024976.3625856352 Coefs: [  95.16733349 1045.86241944] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "### Note, the \"alpha\" parameter defines regularization strength.\n",
    "### Lambda is a reserved word in `Python` -- Thus \"alpha\" instead\n",
    "\n",
    "### An alpha of 0 is equivalent to least-squares regression\n",
    "lr = LinearRegression()\n",
    "reg = Ridge(alpha = 100000)\n",
    "reg0 = Ridge(alpha = 0)\n",
    "\n",
    "# Notice how the consistent sklearn syntax may be used to easily fit many kinds of models\n",
    "for m, name in zip([lr, reg, reg0], [\"LeastSquares\",\"Ridge alpha = 100000\",\"Ridge, alpha = 0\"]):\n",
    "    \n",
    "    m.fit(data[['GrLivArea','YearBuilt']], data['SalePrice'])\n",
    "    print(name, \"Intercept:\", m.intercept_, \"Coefs:\",m.coef_,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Note, in the above example, an alpha of 100,000 is set for the ridge regularization. The reason an alpha value this high is required is because standardization / mean centering of our inputs did not occur, and instead of working with inputs on the order of [-4,4] we are on the interval of [0,2000].  \n",
    "\n",
    "#### Question 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Above, the coefficent around 95/96 corresponds with:\n",
    "### 'a') Living Area\n",
    "### 'b') Year Built\n",
    "### 'c') Sale Price\n",
    "### Assign character associated with your choice as string to ans1\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 9",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Queston 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### True or False:\n",
    "### A larger \"alpha\" corresponds to a greater amount of regularization\n",
    "### assign boolean choice to ans1\n",
    "\n",
    "ans1 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 10",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
