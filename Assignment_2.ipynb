{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Linear Regression - Ridge Regression   \n",
    "-------------------------\n",
    "\n",
    "_Authors: Khal Makhoul, W.P.G.Peterson_  \n",
    "\n",
    "## Project Guide  \n",
    "\n",
    "--------------------\n",
    "- [Project Overview](#overview)  \n",
    "- [Data Review](#data)  \n",
    "- [Coding Ridge Regression](#code)\n",
    "- [Ridge Regression in `sklearn`](#sklearn)  \n",
    "\n",
    "<a id = \"overview\"></a>\n",
    "## Project Overview  \n",
    "-------------\n",
    "#### EXPECTED TIME 2.5 HRS  \n",
    "\n",
    "This assignment will test your ability to code your own version of ridge-regularized regression in `Python`. This assignment draws upon and presupposed the knowledge found in the lectures for Week 2. If ever a theoretical questions arises as to \"why\" we are doing something, please refer back to those lectures.  \n",
    "\n",
    "The assignment also builds upon the work performed in assignment 1 \"*Linear Regression - Least Squares*\". The data used will be the same. Though the last assignment tested your ability to read data into `Pandas` from a `.csv`. Those fundamental processes will not be directly tested here.\n",
    "\n",
    "In coding Ridge Regression you will be asked to:  \n",
    "- Mean center target variable and mean center / standardize observation  \n",
    "- Calculate Ridge Regression weights using linear algebra\n",
    "- Create a  hyperparameter tuning process   \n",
    "\n",
    "**Motivation**: Ridge Regression offers a way to mitigate some of the weaknesses of Least Squares Linear Regression to build more robust models.  \n",
    "\n",
    "**Objectives**: This assignmet will -  \n",
    "\n",
    "- Test `Python` competency and mathematical understandings of Ridge Regression\n",
    "- Begin to introduce the concept of hyper-parameter tuning  \n",
    "\n",
    "**Problem**: Using housing data, we will attempt to predict house price using living area with a regression model.  \n",
    "\n",
    "**Data**: Data comes from [Kaggle's House Prices Dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).  \n",
    "\n",
    "See above link for Description of data.\n",
    "\n",
    "<a id = \"data\"></a>  \n",
    "### Data Exploration\n",
    "\n",
    "Below provides a review of the \"Housing\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### This cell imports the necessary modules and sets a few plotting parameters for display\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)  \n",
    "\n",
    "### Read in the data\n",
    "tr_path = '/Users/davidpeterson/Downloads/PIETHON/Ass1.datasets/train.csv'\n",
    "test_path = '/Users/davidpeterson/Downloads/PIETHON/Ass1.datasets/test.csv'\n",
    "\n",
    "data = pd.read_csv(tr_path)  \n",
    "\n",
    "### The .head() function shows the first few lines of data for perspecitve\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6965"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data.isnull().sum().sum()\n",
    "# values: 'Alley\\nPoolQC\\nFence\\nMiscFeature'\n",
    "data[['Alley']].isnull().sum() # = 1369\n",
    "data[['PoolQC']].isnull().sum() # = 1453\n",
    "data[['Fence']].isnull().sum() # = 1179 \n",
    "data[['MiscFeature']].isnull().sum() # 1406 \n",
    "data.isnull().sum().sum()\n",
    "\n",
    "#sum_of_value = 1369+1453+1179+1406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Which column has the most \"null\" values? assign name as string to ans1.\n",
    "### ### CAPITALIZATION/SPELLING MATTERS e.g. 'Street' != 'street'\n",
    "### How many nulls are in that column? assign number as int to ans2\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "ans1 = 'PoolQC'\n",
    "ans2 = 1453"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 1",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data.plot('GrLivArea', 'SalePrice', kind = 'scatter', marker = 'x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### In building regressions below, a subset of our data will be used.\n",
    "\n",
    "### Practice subsetting a DataFrame below.\n",
    "### Create a DataFrame only containing the \"Street\" and \"Alley\" columns from \n",
    "### the `data` DataFrame.\n",
    "\n",
    "### Assign to 'ans1'\n",
    "### YOUR ANSWER BELOW\n",
    "sub_set = data[['Street', 'Alley']]\n",
    "ans1 = pd.DataFrame(sub_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 2",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"code\"></a>\n",
    "### Coding Ridge Regression   \n",
    "\n",
    "#### Preprocessing\n",
    "Before implementing Ridge Regression, it is important to mean-center our target variable and mean-center and standardize observations. We will do this according to the following:  \n",
    "#### Mean Center Target\n",
    "$$y_{cent} = y_0 - \\bar{y}$$\n",
    "\n",
    "#### Standardize Observations\n",
    "$$X_{std} = \\frac{X_0-\\bar{X}}{s_{X}}$$\n",
    "\n",
    "Where $\\bar{X}$ is the sample mean of X and $s_{X}$ is the sample standard deviation of X.  \n",
    "\n",
    "NOTE: The sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"\n",
    "\n",
    "#### Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Why are the centering / standardization transformations described above important for ridge regression?\n",
    "### 'a') Regression works best when values are unitless\n",
    "### 'b') The transformations makes the regression more interpretable\n",
    "### 'c') Ridge penalizes large coefficients; the transformations make the coefficients of similar scales\n",
    "### 'd') It isn't important\n",
    "\n",
    "### Assign character associated with your choice as a string to ans1\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 3",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 4:\n",
    "$$X_{std} = \\frac{X_0-\\bar{X}}{s_{X}}$$  \n",
    "\n",
    "NOTE: The sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\".  \n",
    "If your answer does not match the example answer, check the default degrees of freedom in your standard deviation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called \"standardize\" \n",
    "### ACCEPT one input, a list of numbers\n",
    "### RETURN a list where those values have been standardized.\n",
    "\n",
    "### To standardize, subtract the mean of the list and divide by standard deviation.\n",
    "### Please use np.std for calculating standard deviation\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def standardize( num_list):\n",
    "    num_mean = np.mean(num_list)\n",
    "    num_sdev = np.std(num_list)\n",
    "    my_values = []\n",
    "    for x in num_list:\n",
    "        top_value = x - num_mean\n",
    "        bot_value = num_sdev\n",
    "        value = top_value/bot_value\n",
    "        my_values.append(value)\n",
    "    \n",
    "    return my_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    num_list = [1,2,3,3,4,4,5,5,5,5,5]\n",
    "    #num_list = [4, 9, 11, 12, 17, 5, 8, 12, 14]\n",
    "    test = standardize(num_list)\n",
    "    ar = np.round(test,2)\n",
    "    ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Standardize the given list of numbers\n",
    "    \n",
    "    Positional arguments:\n",
    "        num_list -- a list of numbers\n",
    "    \n",
    "    Example:\n",
    "        num_list = [1,2,3,3,4,4,5,5,5,5,5]\n",
    "        nl_std = standardize(num_list)\n",
    "        print(np.round(nl_std,2))\n",
    "        #--> np.array([-2.11, -1.36, -0.61, -0.61,  \n",
    "                0.14,  0.14,  0.88,  0.88,  0.88,\n",
    "                0.88,  0.88])\n",
    "    \n",
    "    NOTE: the sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 4",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Below we will create a function which will preprocess  our data by performing:\n",
    "* mean subtraction from $y$,\n",
    "* dimension standardization for $x$.\n",
    "\n",
    "Both according to the equations set out below.\n",
    "\n",
    "#### Mean Center Target\n",
    "$$y_{cent} = y_0 - \\bar{y}$$\n",
    "\n",
    "#### Standardize Observations\n",
    "$$X_{std} = \\frac{X_0-\\bar{X}}{s_{X}}$$  \n",
    "\n",
    "NOTE: the sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"  \n",
    "If your answer does not match the example answer, check the default degrees of freedom in your standard deviation function.\n",
    "\n",
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_regularization(data, y_column_name, x_column_names):\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1515.463698630137"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #fin = preprocess_for_regularization(data,'SalePrice', ['GrLivArea','YearBuilt'])\n",
    "    test = data['GrLivArea'].values\n",
    "    np.mean(test)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = data[['GrLivArea', 'YearBuilt']]\n",
    "ts = st.loc[:,'GrLivArea']\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Perform mean subtraction and dimension standardization on data\n",
    "        \n",
    "    Positional argument:\n",
    "        data -- a pandas dataframe of the data to pre-process\n",
    "        y_column_name -- the name (string) of the column that contains\n",
    "            the target of the training data.\n",
    "        x_column_names -- a *list* of the names of columns that contain the\n",
    "            observations to be standardized\n",
    "        \n",
    "    Returns:\n",
    "        Return a DataFrame consisting only of the columns included\n",
    "        in `y_column_name` and `x_column_names`.\n",
    "        Where the y_column has been mean-centered, and the\n",
    "        x_columns have been mean-centered/standardized.\n",
    "        \n",
    "        \n",
    "    Example:\n",
    "        data = pd.read_csv(tr_path).head()\n",
    "        prepro_data = preprocess_for_regularization(data,'SalePrice', ['GrLivArea','YearBuilt'])\n",
    "        \n",
    "        print(prepro_data) #-->\n",
    "                   GrLivArea  YearBuilt  SalePrice\n",
    "                0  -0.082772   0.716753     7800.0\n",
    "                1  -1.590161  -0.089594   -19200.0\n",
    "                2   0.172946   0.657024    22800.0\n",
    "                3  -0.059219  -1.911342   -60700.0\n",
    "                4   1.559205   0.627159    49300.0\n",
    "    \n",
    "    NOTE: The sample standard deviation should be calculated with 0 \"Delta Degrees of Freedom\"\n",
    "    \n",
    "    If your answer does not match the example answer,\n",
    "    check the default degrees of freedom in your standard deviation function.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 5",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Next, you'll implement the equation for ridge regression using the closed form equation:  \n",
    "\n",
    "$$w_{RR}=(\\lambda+X^TX)^{-1}X^Ty$$  \n",
    "\n",
    "The function will be very similar to the function you wrote for Least Squares Regression with a slightly different matrix to invert.  \n",
    "\n",
    "NB: Many `numpy` matrix functions will be useful. e.g. `np.matmul`, `np.linalg.inv`, `np.ones`, `np.transpose`, and`np.identity`.\n",
    "\n",
    "The main change from Least Squares Regression is that $\\lambda$ is a parameter *we* must set. This is different from the $w$ parameters that we calculate from either closed form or approximation algorithms.  \n",
    "\n",
    "We will address tuning parameters such as $\\lambda$ in the next section.  \n",
    "\n",
    "#### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called \"ridge_regression_weights\"\n",
    "### ACCEPT three inputs:\n",
    "### Two matricies corresponding to the x inputs and y target\n",
    "### and a number (int or float) for the lambda parameter\n",
    "\n",
    "### RETURN a numpy array of regression weights\n",
    "\n",
    "### The following must be accomplished:\n",
    "\n",
    "### Ensure the number of rows of each the X matrix is greater than the number of columns.\n",
    "### ### If not, transpose the matrix.\n",
    "### Ultimately, the y input will have length n.\n",
    "### Thus the x input should be in the shape n-by-p\n",
    "\n",
    "### *Prepend* an n-by-1 column of ones to the input_x matrix\n",
    "\n",
    "### Use the above equation to calculate the least squares weights.\n",
    "### This will involve creating the lambda matrix---\n",
    "### ### a p+1-by-p+1 matrix with the \"lambda_param\" on the diagonal\n",
    "### ### p+1-by-p+1 because of the prepended \"ones\".\n",
    "\n",
    "### NB: Pay close attention to the expected format of the returned\n",
    "### weights. It is different / simplified from Assignment 1.\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def ridge_regression_weights(input_x, output_y, lambda_param):\n",
    "    # x matrix nxp\n",
    "    if (input_x.shape[0] < input_x.shape[1]):\n",
    "        input_x = np.transpose(input_x) \n",
    "    ### *Prepend* an n-by-1 column of ones to the input_x matrix\n",
    "    prepend = np.ones(len(input_x)).reshape(len(input_x),1)\n",
    "    x_prep = np.concatenate((prepend, input_x), axis = 1) \n",
    "    \n",
    "    ### This will involve creating the lambda matrix---\n",
    "    ### ### (p+1 by p+1) matrix with the \"lambda_param\" on the diagonal\n",
    "    lamb = lambda_param*np.identity(len(x_prep[0])) \n",
    "    \n",
    "    # 1st var \n",
    "    x_times = x_prep.T@x_prep # (3,3)\n",
    "    part1 = np.add(lamb,x_times)   \n",
    "    \n",
    "    #inverse\n",
    "    inv = np.linalg.inv(part1)\n",
    "    weights = inv@x_prep.T@my\n",
    "    \n",
    "    return weights\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-576.67947107,   77.45913349,   31.50189177])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        my = np.array([208500, 181500, 223500, \n",
    "                                140000, 250000, 143000, \n",
    "                                307000, 200000, 129900, \n",
    "                                118000])\n",
    "                                \n",
    "        mx = np.array([[1710, 1262, 1786, \n",
    "                                1717, 2198, 1362, \n",
    "                                1694, 2090, 1774, \n",
    "                                1077], \n",
    "                               [2003, 1976, 2001, \n",
    "                                1915, 2000, 1993, \n",
    "                                2004, 1973, 1931, \n",
    "                                1939]])\n",
    "        lm = 10\n",
    "        we = ridge_regression_weights(mx,my,lm)\n",
    "        we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-576.67947107,   77.45913349,   31.50189177])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #Ensure the number of rows of each X matrix is greater than the number of columns.\n",
    "    ### ### If not, transpose the matricies.\n",
    "    ### In particular, the y input should end up as a n-by-1 matrix, and the x input as a n-by-p matrix\n",
    "    \n",
    "    # x matrix nxp \n",
    "    if (mx.shape[0] < mx.shape[1]):\n",
    "        mx = np.transpose(mx)\n",
    "    #mxtrs.shape\n",
    "      \n",
    "    ### *Prepend* an n-by-1 column of ones to the input_x matrix\n",
    "    prepend = np.ones(len(mx)).reshape(len(mx),1)\n",
    "    x_prep = np.concatenate((prepend, mx), axis = 1) # (10,3)\n",
    "    \n",
    "    ### This will involve creating the lambda matrix---\n",
    "    ### ### a 3-by-3  (p+1 by p+1 ) matrix with the \"lambda_param\" on the diagonal\n",
    "    lamb = 10*np.identity(len(x_prep[0])) # (3,3)\n",
    "    \n",
    "    # 1st var \n",
    "    x_times = x_prep.T@x_prep # (3,3)\n",
    "    part1 = np.add(lamb,x_times)\n",
    "    \n",
    "    #inverse\n",
    "    inv = np.linalg.inv(part1)\n",
    "    weights = inv@x_prep.T@my\n",
    "    weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate ridge regression least squares weights.\n",
    "    \n",
    "    Positional arguments:\n",
    "        input_x -- 2-d matrix of input data\n",
    "        output_y -- 1-d numpy array of target values\n",
    "        lambda_param -- lambda parameter that controls how heavily\n",
    "            to penalize large weight values\n",
    "        \n",
    "    Example:\n",
    "        training_y = np.array([208500, 181500, 223500, \n",
    "                                140000, 250000, 143000, \n",
    "                                307000, 200000, 129900, \n",
    "                                118000])\n",
    "                                \n",
    "        training_x = np.array([[1710, 1262, 1786, \n",
    "                                1717, 2198, 1362, \n",
    "                                1694, 2090, 1774, \n",
    "                                1077], \n",
    "                               [2003, 1976, 2001, \n",
    "                                1915, 2000, 1993, \n",
    "                                2004, 1973, 1931, \n",
    "                                1939]])\n",
    "        lambda_param = 10\n",
    "        \n",
    "        rrw = ridge_regression_weights(training_x, training_y, lambda_param)\n",
    "        \n",
    "        print(rrw) #--> np.array([-576.67947107,   77.45913349,   31.50189177])\n",
    "        print(rrw[2]) #--> 31.50189177\n",
    "        \n",
    "    Assumptions:\n",
    "        -- output_y is a vector whose length is the same as the\n",
    "        number of observations in input_x\n",
    "        -- lambda_param has a value greater than 0\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 6",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Selecting the $\\lambda$ parameter\n",
    "\n",
    "For our final function before looking at the `sklearn` implementation of ridge regression, we will create a hyper-parameter tuning algorithm.  \n",
    "\n",
    "In ridge regression, we must pick a value for $\\lambda$. We have some intuition about $\\lambda$ from the equations that define it: small values tend to emulate the results from Least Squares, while large values will reduce the dimensionality of the problem. But the choice of $\\lambda$ can motivated with a more precise quantitative treatment.\n",
    "\n",
    "Eventually, we will look to choose the value of $\\lambda$ that minimizes validation error, which we will determine using $k$-fold cross-validation.\n",
    "\n",
    "For this example here, we will solve a simpler problem: Find a value that minimizes the of the list returned by a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Example of hiden function below:\n",
    "\n",
    "### `hidden` takes a single number as a parameter (int or float) and returns a list of 1000 numbers\n",
    "### the input must be between 0 and 50 exclusive\n",
    "\n",
    "def hidden(hp):\n",
    "    if (hp<=0) or (hp >= 50):\n",
    "        print(\"input out of bounds\")\n",
    "    \n",
    "    nums = np.logspace(0,5,num = 1000)\n",
    "    vals = nums** 43.123985172351235134687934\n",
    "    \n",
    "    user_vals = nums** hp\n",
    "    \n",
    "    return vals-user_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0642517968059461e+213"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    mn = np.mean(hidden(37))\n",
    "    mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Run the above cell and test out the functionality of `hidden`. Remember it takes a single number, between 0 and 50, as an argument\n",
    "\n",
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called \"minimize\"\n",
    "### ACCEPT one input: a function.\n",
    "\n",
    "### That function will be similar to `hidden` created above and available for your exploration.\n",
    "### Like 'hidden', the passed function will take a single argument, a number between 0 and 50 exclusive \n",
    "### and then, the function will return a numpy array of 1000 numbers.\n",
    "\n",
    "### RETURN the value that makes the mean of the array returned by 'passed_func' as close to 0 as possible\n",
    "\n",
    "### Note, you will almost certainly NOT be able to find the number that makes the mean exactly 0\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def minimize( passed_func):\n",
    "    rnge = np.linspace(.01,49.9,10000)  \n",
    "    mylist = []\n",
    "    for x in rnge:\n",
    "        out = passed_func(x)\n",
    "        outmean = np.mean(out)\n",
    "        mylist.append(outmean)\n",
    "        if (outmean == min(mylist) and (outmean > 0 )):\n",
    "            final = x\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        np.mean(hidden(43.123985172351)) #  === 2.8684893433639964e+201 \n",
    "        np.mean(hidden(49.892458738)) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.11927092709271"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #numpy.arange(start, stop, step, dtype)\n",
    "    #arange(.01,49.9,1.1239851e-2, float) # => length 10\n",
    "    rnge = np.linspace(.01,49.9,10000)  \n",
    "    mylist = []\n",
    "    for x in rnge:\n",
    "        out = hidden(x)\n",
    "        outmean = np.mean(out)\n",
    "        mylist.append(outmean)\n",
    "        if (outmean == min(mylist) and (outmean > 0 )):\n",
    "            final = x\n",
    "    final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" You create a range of about 10000 test values using numpy.linspace(.01,49.9,10000) \n",
    "and then you take in some “passed_function” and \n",
    "get the output of that function for every value in your range of test values. \n",
    "Then find the minimum of all the \n",
    "output values and return the test value that produces it. That’s the “minimization” expected. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Find the numeric value that makes the mean of the\n",
    "    output array returned from 'passed_func' as close to 0 as possible.\n",
    "    \n",
    "    Positional Argument:\n",
    "        passed_func -- a function that takes a single number (between 0 and 50 exclusive)\n",
    "            as input, and returns a list of 1000 floats.\n",
    "        \n",
    "    Example:\n",
    "        passed_func = hidden\n",
    "        min_hidden = minimize(passed_func)\n",
    "        print(round(min_hidden,4))\n",
    "        #--> 43.1204 (answers will vary slightly, must be close to 43.123985172351)\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 7",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The above simulates hyper parameter tuning.  \n",
    "\n",
    "In the case of ridge regression, you would be searching lambda parameters to minimize validation error.  \n",
    "\n",
    "The `hidden` function would be analogous to the model building; the returned list analogous to residuals; and the mean of that list analogous to validation error.  \n",
    "\n",
    "See below for an example of using the functions built above that automatically performs hyper-parameter tuning using mean-absolute-deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def lambda_search_func(lambda_param):\n",
    "    \n",
    "    # Define X and y\n",
    "    # with preprocessing\n",
    "    df = preprocess_for_regularization(data.head(50),'SalePrice', ['GrLivArea','YearBuilt'])\n",
    "    \n",
    "    y_true = df['SalePrice'].values\n",
    "    X = df[['GrLivArea','YearBuilt']].values\n",
    "    \n",
    "    # Calculate Weights then use for predictions\n",
    "    weights = ridge_regression_weights(X, y_true, lambda_param )\n",
    "    y_pred = weights[0] + np.matmul(X,weights[1:])\n",
    "    \n",
    "    # Calculate Residuals\n",
    "    resid = y_true - y_pred\n",
    "    \n",
    "    # take absolute value to tune on mean-absolute-deviation\n",
    "    # Alternatively, could use:\n",
    "    # return resid **2-S\n",
    "    # for tuning on mean-squared-error\n",
    "    \n",
    "    return abs(resid)\n",
    "\n",
    "minimize(lambda_search_func)    # --> about 1.4957957957957957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Implementing a k-folds cross-validation strategy will come in later assignments.\n",
    "\n",
    "#### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Why is cross-validation useful?\n",
    "### 'a') to minimize the liklihood of overfitting\n",
    "### 'b') Cross-validation allows us to fit on all our data\n",
    "### 'c') cross-validation standardizes outputs\n",
    "### 'd') cross-validation is not useful\n",
    "### assing the character associated with your choice as a string to ans1\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 8",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"sklearn\"></a>\n",
    "\n",
    "### Ridge Regression in `sklearn`  \n",
    "\n",
    "Below gives the syntax for implementing ridge regression in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "### Note, the \"alpha\" parameter defines regularization strength.\n",
    "### Lambda is a reserved word in `Python` -- Thus \"alpha\" instead\n",
    "\n",
    "### An alpha of 0 is equivalent to least-squares regression\n",
    "lr = LinearRegression()\n",
    "reg = Ridge(alpha = 100000)\n",
    "reg0 = Ridge(alpha = 0)\n",
    "\n",
    "# Notice how the consistent sklearn syntax may be used to easily fit many kinds of models\n",
    "for m, name in zip([lr, reg, reg0], [\"LeastSquares\",\"Ridge alpha = 100000\",\"Ridge, alpha = 0\"]):\n",
    "    \n",
    "    m.fit(data[['GrLivArea','YearBuilt']], data['SalePrice'])\n",
    "    print(name, \"Intercept:\", m.intercept_, \"Coefs:\",m.coef_,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Note, in the above example, an alpha of 100,000 is set for the ridge regularization. The reason an alpha value this high is required is because standardization / mean centering of our inputs did not occur, and instead of working with inputs on the order of [-4,4] we are on the interval of [0,2000].  \n",
    "\n",
    "#### Question 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Above, the coefficent around 95/96 corresponds with:\n",
    "### 'a') Living Area\n",
    "### 'b') Year Built\n",
    "### 'c') Sale Price\n",
    "### Assign character associated with your choice as string to ans1\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 9",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Queston 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### True or False:\n",
    "### A larger \"alpha\" corresponds to a greater amount of regularization\n",
    "### assign boolean choice to ans1\n",
    "\n",
    "ans1 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 10",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
