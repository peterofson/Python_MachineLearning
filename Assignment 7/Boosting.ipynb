{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Boosting\n",
    "#### Implementing Adaptive Boosting with a simple classifier\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "### Assignment Contents:\n",
    "- [Building a Simple Classifier](#Building-a-Simple-Binary-Tree-Classifier)\n",
    "- [Building Adaptive Boosting](#Building-Adaptive-Boosting)\n",
    "- [Census Data](#Census-Data)\n",
    "- [AdaBoost in `sklearn`](#sklearn)\n",
    "\n",
    "#### EXPECTED TIME: 4 HRS\n",
    "\n",
    "### Overview\n",
    "This assignment extends the work done in Assignment 6 with Random Forests.  \n",
    "\n",
    "Initially the assignment is theory heavy, both a simple classifier and an adaptive boosting model will be built from scratch. This will primarily involve creating `Python` implementations of the algorithms found in lectures, both this week and last week. As usual, pre-built versions of these algorithms will be demonstrated at the end of the lesson.  \n",
    "Building the algorithms from scratch help to ensure depth of theoretical knowledge before allowing `Python` and its packages to handle the heavy lifting.\n",
    "\n",
    "### Activities in this Assignment\n",
    "- Create a simple Classifier\n",
    "    - Find potential splits in data\n",
    "    - Find the best split according to entropy\n",
    "    - Create binary predictions given a chosen split\n",
    "- Create an Adaptive Boosting Algorithm\n",
    "    - Create Weights\n",
    "    - Calculate Epsilon and Alph\n",
    "    - Update Weights\n",
    "   \n",
    "- Use Adaptive Boosting to Create Predictions on Census Data\n",
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Building a Simple Binary Tree Classifier\n",
    "\n",
    "Below, pseudo-code for a simple binary tree classifier class is provided.  \n",
    "\n",
    "The structure of this pseudo-code mimics the structure of the `sklearn` classifiers in that it creates \"fit\" and \"predict\" methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Simple_Binary(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X,y):\n",
    "        \"\"\"\n",
    "            1. Find best split in X\n",
    "                - According to entropy\n",
    "            2. After finding split, assign:\n",
    "                - self.col_idx\n",
    "                - self.split_value\n",
    "                - self.left_pred\n",
    "                - self.right_pred\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            1. Make predictions given values calculated\n",
    "                in the `.fit(X,y)` method.\n",
    "            2. return predictions as numpy array.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Instead of building a class like `sklearn`, our `.fit()` and `.predict()` methods will be written as independent `simple_binary_tree_fit()` and `simple_binary_tree_predict()` functions.\n",
    "\n",
    "In `simple_binary_tree_fit`, **3 steps** must be accomplished:  \n",
    "\n",
    "1. Find all of the potential values for splitting.\n",
    "2. Find the column and split_value that results in the lowest entropy.\n",
    "3. Given that \"best split\", determine which predictions should be made when data is \"<=\" and \">\" that split value.  \n",
    "\n",
    "The below provides framework for returning the column and split value that yeilds the lowest entropy, and the predictions indicated by that split.  \n",
    "After correctly defining `find_splits()`, `ent_from_split()`, and `pred_from_split()` -- marked with \"`### <------`\" -- the `simple_binary_tree_fit()` function should work.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def simple_binary_tree_fit(X,y):\n",
    "    \"\"\"\n",
    "    \n",
    "    Positional arguments -\n",
    "        X -- a numpy array of numeric observations:\n",
    "            Assume rows are separate observations, columns are features\n",
    "        y -- a numpy array of binary labels:\n",
    "            *Assume labels are 1 for \"True\" and 0 for \"False\"*\n",
    "            \n",
    "    1. Find best split in X\n",
    "        - According to entropy\n",
    "    2. After finding split, return:\n",
    "        - col_idx - index of column used to split data\n",
    "        - split_value - value upon which data is split\n",
    "        - left_pred - The prediction for observation <= split_value\n",
    "        - right_pred - The prediciton for observation > split_value\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # create variable \"best_split\" which will hold:\n",
    "    # (col_number, split_value, entropy)\n",
    "    best_split = (-1,-1,1)\n",
    "    \n",
    "    # loop through each column in X, keeping track of the column index.\n",
    "    # # # Note, taking the transpose of X -- X.T -- yeilds columns in this \"for\" loop\n",
    "    for col_idx, col in enumerate(X.T):\n",
    "        \n",
    "        # Find potential split values within column using `find_splits(col)`\n",
    "        splits = find_splits(col) ### <------\n",
    "        \n",
    "        # For each split, calculate entropy\n",
    "        for s in splits:\n",
    "            ent = ent_from_split(col, s, y) ### <------\n",
    "            \n",
    "            # Check if calculated entropy is less than previous \"best\"\n",
    "            if ent < best_split[2]:\n",
    "                best_split = (col_idx, s, ent)\n",
    "    \n",
    "    # Now, the \"best split\" has been found.\n",
    "    # create \"left\" and \"right\" predictions for the best_split\n",
    "    # The \"left\" predictions is for when `observation` <= `split_value`\n",
    "    # The \"right\" prediction is for when `observation` > `split_value`\n",
    "    # Each prediction will either be 1 for \"True\" or 0 for \"False\"\n",
    "    \n",
    "    left_pred, right_pred = pred_from_split(X, y, *best_split[:2]) ### <------\n",
    "    \n",
    "    col_idx, split_value = best_split[:2]\n",
    "    \n",
    "    # return:\n",
    "    # - the index of the column to split on.\n",
    "    # - the value to split that column on\n",
    "    # - the prediction for rows with observations in that column less than or equal to the split\n",
    "    # - the prediction for rows with observations in that column greater than the split\n",
    "    \n",
    "    return col_idx, split_value, left_pred, right_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 1\n",
    "\n",
    "Build the `find_splits()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called `find_splits`.\n",
    "### ACCEPT a 1-dimensional numpy array as input.\n",
    "### RETURN a numpy.array of \"split values\"\n",
    "\n",
    "### \"Split values\" are the mid-points between the values in the sorted list of unique values.\n",
    "\n",
    "### e.g., Input of np.array([1, 3, 2, 3, 4, 6])\n",
    "### Yields a sorted-unique list of: np.array([1, 2, 3, 4, 6])\n",
    "### Then the \"splits\" in between those values will be: np.array([1.5, 2.5, 3.5, 5])\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def find_splits(col):\n",
    "    \"\"\"\n",
    "    Calculate and return all possible split values given a column of numeric data\n",
    "    \n",
    "    Positional argument:\n",
    "        col -- a 1-dimensional numpy array, corresponding to a numeric\n",
    "            predictor variable.\n",
    "    \n",
    "    Example:\n",
    "        col = np.array([0.5, 1. , 3. , 2. , 3. , 3.5, 3.6, 4. , 4.5, 4.7])\n",
    "        splits  = find_splits(col)\n",
    "        print(splits) # --> np.array([0.75, 1.5, 2.5, 3.25, 3.55, 3.8, 4.25, 4.6])\n",
    "        \n",
    "    \"\"\"\n",
    "    # Sort the array from smallest to largest \n",
    "    new_col = sorted(col)\n",
    "    fnl = []\n",
    "    splits = []\n",
    "    cnt=0\n",
    "    # Sorted unique list of numbers - repeated values removed \n",
    "    for x in new_col:\n",
    "        if (x not in fnl):\n",
    "            fnl.append(x)\n",
    "    # Evaluate each element and find the mid-point between them by using a while loop \n",
    "    while (cnt < len(fnl)-1):\n",
    "        avg = (fnl[cnt]+fnl[cnt+1])/2\n",
    "        splits.append(avg)\n",
    "        cnt+=1\n",
    "    # Return an array \n",
    "    splits = np.array(splits)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75, 1.5 , 2.5 , 3.25, 3.55, 3.8 , 4.25, 4.6 ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    col = np.array([0.5, 1. , 3. , 2. , 3. , 3.5, 3.6, 4. , 4.5, 4.7])\n",
    "    find_splits(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75, 1.5 , 2.5 , 3.25, 3.55, 3.8 , 4.25, 4.6 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    ### TESTING for above function \n",
    "    col = np.array([0.5, 1. , 3. , 2. , 3. , 3.5, 3.6, 4. , 4.5, 4.7])\n",
    "    new_col = sorted(col)\n",
    "    fnl = []\n",
    "    splits = []\n",
    "    cnt=0\n",
    "    \n",
    "    # Sorted unique list of numbers - repeated values removed \n",
    "    for x in new_col:\n",
    "        if (x not in fnl):\n",
    "            fnl.append(x)\n",
    "    \n",
    "    while (cnt < len(fnl)-1):\n",
    "        avg = (fnl[cnt]+fnl[cnt+1])/2\n",
    "        splits.append(avg)\n",
    "        cnt+=1\n",
    "    splits = np.array(splits)\n",
    "    splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 1",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Equation for Entropy for binary classification at binary split:\n",
    "The entropy at a node containing only two classes is calculated by:\n",
    "\n",
    "$Entropy(node) = -p_{class1}*log_2(p_{class1}) + -p_{class2}*log_2(p_{class2})$  \n",
    "\n",
    "Suppose a node contains the observations [1,0,1,1]. Then:  \n",
    "\n",
    "$Entropy(node) = -p_{class1}*log_2(p_{class1}) + -p_{class2}*log_2(p_{class2})$  \n",
    "\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = -.75*log_2(.75) + -.25*log_2(.25)$\n",
    "\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = .311 + .5$\n",
    "\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\approx .811$  \n",
    "\n",
    "\n",
    "This calculation is already programmed into the supplied `entropy()` function below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "def entropy(class1_n, class2_n):\n",
    "    # If all of one category, log2(0) does not exist,\n",
    "    # and entropy = 0\n",
    "    if (class1_n == 0) or (class2_n == 0):\n",
    "        return 0\n",
    "\n",
    "    # Find total number of observations \n",
    "    total = class1_n + class2_n\n",
    "\n",
    "    # find proportion of both classes\n",
    "    class1_proprtion = class1_n/total\n",
    "    class2_proportion = class2_n/total\n",
    "\n",
    "    # implement entropy function\n",
    "    return  sum([-1 * prop * np.log2(prop)\n",
    "                 for prop in [class1_proprtion, class2_proportion] ])\n",
    "\n",
    "print(entropy(3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "The entropy of a split is:  \n",
    "$Entropy(split) = p_{node1} * Entropy(node1)+ p_{node2}* Entropy(node2)$  \n",
    "\n",
    "\n",
    "Where $p_{node}$ is the proportion of observations at that node\n",
    "\n",
    "Suppose:  \n",
    "Node 1 contains the observations - [1,0,1,1]  \n",
    "Node 2 contains the observations - [0,0,0,1,1,0]\n",
    "\n",
    "Then:  \n",
    "$Entropy(split) = p_{node1} * Entropy(node1)+ p_{node2}* Entropy(node2)$  \n",
    "\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = .4 *Entropy(node1) + .6 * Entropy(node2)$\n",
    "\n",
    "--------------------------------  \n",
    "\n",
    "For our purposes, the two classes in each node will be defined by:\n",
    "1. Observations with values less than or equal to the split value  \n",
    "2. Observations with values greater than the split value.\n",
    "\n",
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called `ent_from_split`\n",
    "### ACCEPT three inputs:\n",
    "### 1. A numpy array of values\n",
    "### 2. A value on which to split the values in the first array, (into two groups; <= and >)\n",
    "### 3. Labels for the observations corresponding to each value in the first array.\n",
    "### ### Assume the labels are \"0\"s and \"1\"s\n",
    "\n",
    "### RETURN the entropy resulting from that split: a float between 0 and 1.\n",
    "\n",
    "### Feel free to use the `entropy()` function defined above\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def ent_from_split(col, split_value, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the entropy of a split.\n",
    "    \n",
    "    Positional arguments:\n",
    "        col -- a 1-dimensional numpy array, corresponding to a numeric\n",
    "            predictor variable.\n",
    "        split_value --  number, defining where the spliting should occur\n",
    "        labels -- a 1-dimensional numpy array, corresponding to the class\n",
    "            labels associated with the observations in `col`.\n",
    "            assume they will be \"0\"s and \"1\"s\n",
    "    Example:\n",
    "        col = np.array([1,1,2,2,3,3,4])\n",
    "        split = 2.5\n",
    "        labels = np.array([0,1,0,0,1,0,1])\n",
    "        \n",
    "        ent = ent_from_split(col, split, labels)\n",
    "        \n",
    "        print(ent) # --> 0.8571428571428571\n",
    "    \n",
    "    \"\"\"\n",
    "    # Seperate the labels according to the classes\n",
    "    nd1_labels = labels[col <= split] \n",
    "    nd2_labels = labels[col > split] \n",
    "    # Find out how many labels (0's and 1's) are in each node \n",
    "    # First node \n",
    "    c1_ones = np.count_nonzero(nd1_labels)\n",
    "    c1_zeros = len(nd1_labels) - c1_ones\n",
    "    # Second node \n",
    "    c2_ones = np.count_nonzero(nd2_labels)\n",
    "    c2_zeros = len(nd2_labels) - c2_ones \n",
    "    # Calculate the entropy using the given equation \n",
    "    ent_1 = entropy(c1_ones, c1_zeros)\n",
    "    ent_2 = entropy(c2_ones, c2_zeros)\n",
    "    # Calculate the proportions \n",
    "    prop1 = len(nd1_labels)/len(labels)\n",
    "    prop2 = len(nd2_labels)/len(labels)\n",
    "    # Use equation for split entropy's     \n",
    "    ent = float(ent_1*prop1) + (prop2*ent_2)\n",
    "    return ent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    col = np.array([1,1,2,2,3,3,4])\n",
    "    split = 2.5\n",
    "    labels = np.array([0,1,0,0,1,0,1])\n",
    "    ent_from_split(col, split, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 2",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 3\n",
    "\n",
    "Creating predictions from the observed majority class at each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called `pred_from_split`\n",
    "### ACCEPT four inputs:\n",
    "### 1. a numpy array of observations\n",
    "### 2. a numpy array of labels: 0's and 1's\n",
    "### 3. a column index\n",
    "### 4. a value to split that column specified by the index\n",
    "\n",
    "### RETURN a tuple of (left_pred, right_pred) where:\n",
    "### left_pred is the majority class of labels where observations are <= split_value\n",
    "### right_pred is the majority class of labels where observations are > split_value\n",
    "\n",
    "### If the split yeilds equal number of observations of each class in BOTH nodes,\n",
    "### ### let both `left_pred` and `right_pred` be 1.\n",
    "### If the split yeilds equal number of observations of each class in ONLY ONE node,\n",
    "### ### predict the opposite of the other node. e.g.\n",
    "\n",
    "### ### node 1    |   node 2\n",
    "### ###  c1  | c2 |  c1 | c2\n",
    "### ###  5  | 4   |  3  |  3\n",
    "\n",
    "### The prediction for node 1 would be \"class 1\".\n",
    "### Because of the equal numbers of each class in node 2,\n",
    "### the prediction for node 2 would be the opposite of the node 1 prediction.\n",
    "### e.g. the prediction for node 2 would be \"class 2\"\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def pred_from_split(X, y, col_idx, split_value):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return predictions for the nodes defined by the given split.\n",
    "    \n",
    "    Positional argument:\n",
    "        X -- a 2-dimensional numpy array of predictor variable observations.\n",
    "            rows are observations, columns are features.\n",
    "        y -- a 1-dimensional numpy array of labels, associated with observations\n",
    "             in X.\n",
    "        col_idx -- an integer index, such that X[:,col_idx] yeilds all the observations\n",
    "            of a single feature.\n",
    "        split_value -- a numeric split, such that the values of X[:,col_idx] that are\n",
    "            <= split_value are in the left node. Those > split_value are in the right node.\n",
    "    \n",
    "    Example:\n",
    "        X = np.array([[0.5, 3. ], [1.,  2. ], [3.,  0.5],\n",
    "                      [2.,  3. ], [3.,  4. ]])\n",
    "            \n",
    "        y = np.array([ 1, 1, 0, 0, 1])\n",
    "        \n",
    "        col_idx = 0\n",
    "        \n",
    "        split_value = 1.5\n",
    "        \n",
    "        pred_at_nodes = pred_from_split(X, y, col_idx, split_value)\n",
    "        print(pred_at_nodes) # --> (1, 0)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    left_pred = 0\n",
    "    right_pred = 0\n",
    "    \n",
    "    return (left_pred, right_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 3",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 4\n",
    "Creating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called \"simple_binary_tree_predict\"\n",
    "### ACCEPT five inputs:\n",
    "### 1. A numpy array of observations\n",
    "### 2. A column index\n",
    "### 3. A value to split the column specified by the index\n",
    "### 4/5. Two values, 1 or 0, denoting the predictions at left and right nodes\n",
    "\n",
    "### RETURN a numpy array of predictions for each observation\n",
    "\n",
    "### Predictions are created for each row in x:\n",
    "### 1. For a row in X, find the value in the \"col_idx\" column\n",
    "### 2. Compare to \"split_value\"\n",
    "### 3. If <= \"split_value\", predict \"left_pred\"\n",
    "### 4. Else predict \"right_pred\"\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def simple_binary_tree_predict(X, col_idx, split_value, left_pred, right_pred):\n",
    "    \"\"\"\n",
    "    Create an array of predictions built from: observations in one column of X,\n",
    "        a given split value, and given predictions for when observations\n",
    "        are less-than-or-equal-to that split or greater-than that split value\n",
    "        \n",
    "    Positional arguments:\n",
    "        X -- a 2-dimensional numpy array of predictor variable observations.\n",
    "            rows are observations, columns are different features\n",
    "        col_idx -- an integer index, such that X[:,col_idx] yeilds all the observations\n",
    "            in a single feature.\n",
    "        split_value -- a numeric split, such that the values of X[:,col_idx] that are\n",
    "            <= split_value are in the left node, and those > are in the right node.   \n",
    "        left_pred -- class (0 or 1), that is predicted when observations\n",
    "            are less-than-or-equal-to the split value\n",
    "        right_pred -- class (0 or 1), that is predicted when observations\n",
    "            are greater-than the split value\n",
    "            \n",
    "    Example:\n",
    "        X = np.array([[0.5, 3. ], [1.,  2. ], [3.,  0.5],\n",
    "                [2.,  3. ], [3.,  4. ]])\n",
    "        col_idx = 0\n",
    "        split_value = 1.5\n",
    "        left_pred = 1\n",
    "        right_pred = 0\n",
    "\n",
    "        preds = simple_binary_tree_predict(X, col_idx, split_value, left_pred, right_pred)\n",
    "\n",
    "        print(preds) #--> np.array([1,1,0,0,0])\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array([])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 4",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "At this point, we have a functioning binary-tree classifier that can be fit on data, and then given that fit, make predictions on out-of-sample data.  \n",
    "\n",
    "However, our ultimate goal is creation of an Adaptive Boosting algorithm.  \n",
    "\n",
    "\n",
    "Our Adaptive Boosting algorithm's prediction for out of sample data will be:  \n",
    "\n",
    "$$f_{boost}(x_0) = sign(\\sum_{t=1}^T\\alpha_tf_t(X_0))$$  \n",
    "\n",
    "The alpha is equal to:\n",
    "$$\\alpha_t = \\frac12ln(\\frac{1-\\epsilon_t}{\\epsilon_t})$$  \n",
    "\n",
    "The Epsilon is equal to:  \n",
    "\n",
    "$$\\epsilon_t = \\sum_{i=1}^nw_t(i)\\mathbb{1}\\{y_i\\ne f_t(x_i)\\}$$  \n",
    "\n",
    "Where all weights starts at$\\frac1n$  \n",
    "\n",
    "And weights update by:\n",
    "$$w_{t+1}(i) = \\frac{\\hat{w}_{t+1}(i)}{\\sum_j\\hat{w}_{t+1}(j)}$$  \n",
    "\n",
    "Where:\n",
    "$$\\hat{w}_{t+1}(i) = w_t(i)e^{-\\alpha_ty_if_t(x_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "While we could use the `simple_binary_tree` functions created above, in the interest of speed, we will use sklearn's `DecisionTreeClassifier` as a the simple predictor to boost.\n",
    "\n",
    "The below gives a short example of using `DecisionTreeClassifier` that:  \n",
    "\n",
    "- Splits toy data in two\n",
    "- Builds two Trees each on 1/2 of data\n",
    "- Saves each tree with an associated \"alpha\" in a dictionary (As will be done in boosting)\n",
    "- Creates predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### This helper function will return an instance of a `DecisionTreeClassifier` with\n",
    "### our specifications - split on entropy, and grown to depth of 1.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def simple_tree():\n",
    "    return DecisionTreeClassifier(criterion = 'entropy', max_depth= 1)\n",
    "\n",
    "\n",
    "### Our example dataset, inspired from lecture\n",
    "pts = [[.5, 3,1],[1,2,1],[3,.5,-1],[2,3,-1],[3,4,1],\n",
    " [3.5,2.5,-1],[3.6,4.7,1],[4,4.2,1],[4.5,2,-1],[4.7,4.5,-1]]\n",
    "\n",
    "df = pd.DataFrame(pts, columns = ['x','y','classification'])\n",
    "\n",
    "# Plotting by category\n",
    "\n",
    "b = df[df.classification ==1]\n",
    "r = df[df.classification ==-1]\n",
    "plt.figure(figsize = (4,4))\n",
    "plt.scatter(b.x, b.y, color = 'b', marker=\"+\", s = 400)\n",
    "plt.scatter(r.x, r.y, color = 'r', marker = \"o\", s = 400)\n",
    "plt.title(\"Categories Denoted by Color/Shape\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"df:\\n\",df, \"\\n\")\n",
    "\n",
    "### split out X and y\n",
    "X = df[['x','y']]\n",
    "\n",
    "# Change from -1 and 1 to 0 and 1\n",
    "y = np.array([1 if x == 1 else 0 for x in df['classification']])\n",
    "\n",
    "### Split data in half\n",
    "X1 = X.iloc[:len(X.index)//2, :]\n",
    "X2 = X.iloc[len(X.index)//2:, :]\n",
    "\n",
    "y1 = y[:len(y)//2]\n",
    "y2 = y[len(X)//2:]\n",
    "\n",
    "\n",
    "### Fit classifier to both sets of data, save to dictionary:\n",
    "\n",
    "tree_dict = {}\n",
    "\n",
    "tree1 = simple_tree()\n",
    "tree1.fit(X1,y1)\n",
    "print(\"threshold:\", tree1.tree_.threshold[0], \"feature:\", tree1.tree_.feature[0])\n",
    "\n",
    "### made up alpha, for example\n",
    "alpha1 = .6\n",
    "tree_dict[1] = (tree1, alpha1)\n",
    "\n",
    "tree2 = simple_tree()\n",
    "tree2.fit(X2,y2)\n",
    "print(\"threshold:\", tree2.tree_.threshold[0], \"feature:\" ,tree2.tree_.feature[0])\n",
    "\n",
    "### made up alpha, again.\n",
    "alpha2 = .35\n",
    "\n",
    "tree_dict[2] = (tree2, alpha2)\n",
    "\n",
    "### Create predictions using trees stored in dictionary\n",
    "print(\"\\ntree1 predictions on all elements:\", tree_dict[1][0].predict(X))\n",
    "print(\"tree2 predictions on all elements:\", tree_dict[2][0].predict(X))\n",
    "\n",
    "### Showing Ent\n",
    "print(\"\\nEntropy of different splits for observations 5-9\")\n",
    "print(\"Col 1, @ 3.35:\", ent_from_split(X2.iloc[:,1].values,3.35, y2))\n",
    "print(\"Col 0, # 4.25:\", ent_from_split(X2.iloc[:,0].values, 4.25, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Running the above cell a number of times, you might notice that the threshold and feature for `tree2` change.  \n",
    "\n",
    "At the bottom of the cell, the entropy for two different splits is shown to be identical. This is unlikely to happen with \"real\" data.  \n",
    "\n",
    "#### Bootstrapping\n",
    "\n",
    "Taking a bootstrap sample in adaptive boosting requires selecting observation with pre-defined probabilities.  \n",
    "\n",
    "\n",
    "Below offers an example of selecting random numbers with numpy given pre-defined probabilities.  \n",
    "\n",
    "This will be done with `np.random.choice()`, documentation below:\n",
    "![choice Documentation](./assets/npChoicProbs.PNG)  \n",
    "\n",
    "Try running the below cell a few times, to gain a sense of how `.choice()` works while passing a value for the `<p>` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_from = list(range(5))\n",
    "print(\"selecting from:\", select_from, \"\\n\")\n",
    "\n",
    "### Implement 1/n weights (which is the np.random.choice default)\n",
    "### Also note:\n",
    "### replace = True (default, used for boot-strapping)\n",
    "### size = len(array) - This will be the sample size used in our algorithms.\n",
    "\n",
    "print(\"Equal Weights:\\n\",\n",
    "    np.random.choice(select_from,\n",
    "                size = len(select_from),\n",
    "                replace = True,\n",
    "                p = np.array([.2,.2,.2,.2,.2,])\n",
    "                )\n",
    ")\n",
    "\n",
    "### Now, using uneven weights\n",
    "\n",
    "print(\"\\nWeights of [.9,.05,.03,.02,0]:\\n\",\n",
    "    np.random.choice(select_from,\n",
    "                size = len(select_from),\n",
    "                p = np.array([.9,.05,.03,.02,0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Building Adaptive Boosting\n",
    "\n",
    "Below Gives the outline of the fitting process for the adaptive boosting algorithm.  \n",
    "\n",
    "Again, the functions next to \"`###<------`\" will be created in the exercises below. They include:  \n",
    "\n",
    "- `default_weights()`\n",
    "- `calc_epsilon()`\n",
    "- `calc_alpha()`\n",
    "- `update_weights()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def simple_adaboost_fit(X,y, n_estimators):\n",
    "    \"\"\"\n",
    "    Positional arguments :\n",
    "        X -- a numpy array of numeric observations:\n",
    "            rows are observations, columns are features\n",
    "        y -- a numpy array of binary labels:\n",
    "            *Assume labels are 1 for \"True\" and 0 for \"False\"*\n",
    "        estimator -- a model capable of binary classification, implementing\n",
    "            the `.fit()` and `.predict()` methods.\n",
    "        n_estimators -- The number of estimators to fit.\n",
    "\n",
    "    Steps:\n",
    "        1. Create probability weights for selection during boot-straping.\n",
    "        2. Create boot-strap sample of observations according to weights\n",
    "        3. Fit estimator model with boot-strap sample.\n",
    "        4. Calculate model error: epsilon\n",
    "        5. Calculate alpha to associate with model\n",
    "        6. Re-calculate probability weights\n",
    "        7. Repeat 2-6 unil creation of n_estimators models. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def simple_tree():\n",
    "        return DecisionTreeClassifier(criterion = 'entropy', max_depth= 1)\n",
    "    \n",
    "    # Create default weights array where all are equal to 1/n\n",
    "    weights = default_weights(len(y)) ### <------\n",
    "    \n",
    "    est_dict = {}\n",
    "    for i in range(n_estimators):\n",
    "        # Create bootstrap sample\n",
    "        bs_X, bs_y = boot_strap_selection(X, y, weights)\n",
    "        \n",
    "        mod = simple_tree()\n",
    "        mod.fit(bs_X, bs_y)\n",
    "        \n",
    "        # Note: Predicting on all values of X, NOT boot-strap\n",
    "        preds = mod.predict(X)\n",
    "        \n",
    "        epsilon = calc_epsilon(y, preds, weights) ### <------\n",
    "        alpha = calc_alpha(epsilon) ### <------\n",
    "        \n",
    "        # Note that the i+1-th model will be keyed to the int i,\n",
    "        # and will store a tuple of the fit model and the alpha value\n",
    "        est_dict[i] = (mod, alpha)\n",
    "        \n",
    "        weights = update_weights(weights, alpha, y, preds) ### <------\n",
    "    \n",
    "    return est_dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 5\n",
    "Creating vector of default weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called 'default_weights\n",
    "### ACCEPT a single integer, `n`,  as input\n",
    "### RETURN default weights; a numpy array of lenth n, where each value is equal to 1/n\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def default_weights(n):\n",
    "    \"\"\"\n",
    "    Create the default list of weights, a numpy array of length n\n",
    "    with each value equal to 1/n\n",
    "    \n",
    "    Example:\n",
    "        n = 10\n",
    "        dw = default_weights(n)\n",
    "        print(dw) #--> np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
    "        \n",
    "    \"\"\"\n",
    "    return np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 5",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### boot_strap_selection  \n",
    "\n",
    "Below, the \"`boot_strap_selection`\" algorithm is provided. The function creates a boot-strap sample given the passed-in weights.  \n",
    "\n",
    "Example given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def boot_strap_selection(X, y, weights):\n",
    "    \"\"\"\n",
    "    Create and return a boot-strapped sample of the given data,\n",
    "    According to the provided weights.\n",
    "    \n",
    "    Positional Arguments:\n",
    "        X -- a numpy array, corresponding to the matrix of x-observations\n",
    "        y -- a numpy array, corresponding to a vector of y-labels\n",
    "            All either 0 or 1\n",
    "        weights -- a numpy array, corresponding to the rate at which the observations\n",
    "            should be sampled for the boot-strap. \n",
    "            \n",
    "    Example: \n",
    "    \n",
    "        X = np.array([[1,1],[2,2],[3,3],[4,4],[5,5]])\n",
    "        y = np.array([1,0,1,0,1])\n",
    "        weights = np.array([.35,.1,.1,.35,.1])\n",
    "        \n",
    "        print(boot_strap_selection(X,y, weights))\n",
    "        #-->(\n",
    "            np.array([[4, 4],\n",
    "                   [2, 2],\n",
    "                   [4, 4],\n",
    "                   [5, 5],\n",
    "                   [5, 5]]),\n",
    "            np.array([0, 0, 0, 1, 1]))\n",
    "        ### Actual results will vary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Take random sample of indicies, with replacement\n",
    "    bss_indicies = np.random.choice(range(len(y)), size = len(y), p = weights)\n",
    "    \n",
    "    # Subset arrays with indicies\n",
    "    return X[bss_indicies,:], y[bss_indicies]\n",
    "\n",
    "### Example of use\n",
    "X = np.array([[1,1],[2,2],[3,3],[4,4],[5,5]])\n",
    "y = np.array([1,0,1,0,1])\n",
    "weights = np.array([.35,.1,.1,.35,.1])\n",
    "\n",
    "print(boot_strap_selection(X,y, weights))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 6\n",
    "Calculating Epsilon\n",
    "\n",
    "The Epsilon is equal to:  \n",
    "$$\\epsilon_t = \\sum_{i=1}^nw_t(i)\\mathbb{1}\\{y_i\\ne f_t(x_i)\\}$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called `calc_epsilon` \n",
    "### ACCEPT three inputs:\n",
    "### 1. The True labels\n",
    "### 2. The Predicted labels\n",
    "### 3. The current Weights\n",
    "\n",
    "### RETURN the epsilon value, calculated according to the above equation.\n",
    "### ### Will be a float between 0 and 1\n",
    "\n",
    "### The epsilon is the sum of the weights where the true-label DOES NOT EQUAL the predicted-label\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def calc_epsilon(y_true, y_pred, weights):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the value of epsilon, given the above equation \n",
    "    \n",
    "    Positional Arguments:\n",
    "        y_true -- An np.array of 1's and 0's corresponding to whether each observation is\n",
    "            a member of class 1 or class 2\n",
    "        y_pred -- An np.array of 1's and 0's corresponding to whether each observation was\n",
    "            predicted to be a member of class 1 or class 2\n",
    "        weights -- An np.array of floats corresponding to each observation's weight. \n",
    "            All the weights will sum up to 1.\n",
    "            \n",
    "    Example:\n",
    "        y_true = np.array([1,0,1,1,0])\n",
    "        y_pred = np.array([0,0,0,1,0])\n",
    "        weights = np.array([.4,.4,.1,.05,.05])\n",
    "        \n",
    "        ep = calc_epsilon(y_true, y_pred, weights)\n",
    "        \n",
    "        print(ep) # --> .5\n",
    "        \n",
    "    Assumptions:\n",
    "        Assume both the true labels and the predictions are both all 0's and 1's.\n",
    "    \"\"\"\n",
    "    return float()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 6",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 7\n",
    "Calculating alpha.\n",
    "\n",
    "Alpha is equal to:\n",
    "$$\\alpha_t = \\frac12ln(\\frac{1-\\epsilon_t}{\\epsilon_t})$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called `calc_alpha`\n",
    "### ACCEPT a non-negative float (epsilon) as input\n",
    "### RETURN the alpha (float) calculated using the equation above.\n",
    "### HOWEVER, if epsilon equals 0, return np.inf\n",
    "\n",
    "### NB: np.log() calculates the natural log\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def calc_alpha(epsilon):\n",
    "    \"\"\"\n",
    "    Calculate the alpha value given the epsilon observed from a model\n",
    "    \n",
    "    Positional Argument:\n",
    "        epsilon -- The epsilon value calculated from a particular model\n",
    "    Example:\n",
    "        ep = .4\n",
    "        alpha = calc_alpha(ep)\n",
    "        print(alpha) # --> 0.2027325540540821\n",
    "    \"\"\"\n",
    "    \n",
    "    return float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 7",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 8\n",
    "Updating weights\n",
    "\n",
    "To update weights:\n",
    "$$w_{t+1}(i) = \\frac{\\hat{w}_{t+1}(i)}{\\sum_j\\hat{w}_{t+1}(j)}$$  \n",
    "\n",
    "Where:\n",
    "$$\\hat{w}_{t+1}(i) = w_t(i)e^{-\\alpha_ty_if_t(x_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function \"update_weights\"\n",
    "### ACCEPT four inputs:\n",
    "### 1. A numpy array of a weight vector\n",
    "### 2. An alpha value (float)\n",
    "### 3/4. numpy arrays of true labels and predicted labels vectors.\n",
    "\n",
    "### NB: Labels will need to be converted from 0s and 1s to -1s and 1s\n",
    "\n",
    "### RETURN an updated array of weights, according to equation above.\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def update_weights(weights, alpha, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Create an updated vector of weights according to the above equations\n",
    "    \n",
    "    Positional Arguments:\n",
    "        weights -- a 1-d numpy array of positive floats, corresponding to \n",
    "            observation weights\n",
    "        alpha -- a positive float\n",
    "        y_true -- a 1-d numpy array of true labels, all 0s and 1s\n",
    "        y_pred -- a 1-d numpy array of labels predicted by the last model;\n",
    "             all 0s and 1s. \n",
    "    \n",
    "    Example:\n",
    "        y_true = np.array([1,0,1,1,0])\n",
    "        y_pred = np.array([0,0,1,1,1])\n",
    "        weights = np.array([.4,.4,.1,.05,.05])\n",
    "        alpha = 0.10033534773107562\n",
    "        \n",
    "        print(update_weights(weights, alpha, y_true, y_pred))\n",
    "        #-->np.array([0.44444444 0.36363636 0.09090909 0.04545455 0.05555556])\n",
    "        \n",
    "    \"\"\"\n",
    "    return np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 8",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "With the above functions created, the \"`simple_adaboost_fit()`\" method should work correctly.  \n",
    "\n",
    "`simple_adaboost_fit()` returns a dictionary where the keys are 0 through n-1 where n is the `n_estimators` from the function signature.  \n",
    "\n",
    "The values of the dictionaries are (model, alpha) where `model`is a `DecisionTreeClassifier`, and `alpha` is a float.  \n",
    "\n",
    "#### Question 9\n",
    "Creating a Prediction from boosted trees\n",
    "\n",
    "Our prediction will be:  \n",
    "\n",
    "$$f_{boost}(x_0) = sign(\\sum_{t=1}^T\\alpha_tf_t(X_0))$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called `predict`\n",
    "\n",
    "### ACCEPT two inputs:\n",
    "### 1. a 2-d numpy array of x-obervations\n",
    "### 2. a dictionary that contains classifiers and alphas (described more below and above)\n",
    "\n",
    "### Combine the models as in the manner described in the equation above\n",
    "### to create predictions for the observations.\n",
    "\n",
    "### RETURN a 1-d numpy array of observations (all 0s and 1s)\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def predict(X, est_dict):\n",
    "    \"\"\"\n",
    "    Create a np.array list of predictions for all of the observations in x,\n",
    "    according to the above equation.\n",
    "    \n",
    "    Positional Arguments:\n",
    "        X -- a 2-d numpy array of X observations. Features in columns, \n",
    "            observations in rows.\n",
    "        est_dict -- a dictionary consists of keys 0 through n with tuples as values\n",
    "            The tuples will be (<mod>, alpha), where alpha is a float, and \n",
    "            <mod> is a sklearn DecisionTreeClassifier\n",
    "    Example:\n",
    "    \n",
    "        ### Our example dataset, inspired from lecture\n",
    "        pts = [[.5, 3,1],[1,2,1],[3,.5,0],[2,3,0],[3,4,1],\n",
    "         [3.5,2.5,0],[3.6,4.7,1],[4,4.2,1],[4.5,2,0],[4.7,4.5,0]]\n",
    "\n",
    "        df = pd.DataFrame(pts, columns = ['x','y','classification'])\n",
    "        \n",
    "        ### split out X and labels\n",
    "        X = df[['x','y']]\n",
    "        y = df['classification']\n",
    "        ### Split data in half\n",
    "        X1 = X.iloc[:len(X.index)//2, :]\n",
    "        X2 = X.iloc[len(X.index)//2:, :]\n",
    "\n",
    "        y1 = y[:len(y)//2]\n",
    "        y2 = y[len(X)//2:]\n",
    "\n",
    "\n",
    "        ### Fit classifiers to both sets of data, save to dictionary:\n",
    "        \n",
    "        ### Tree-creator helper function\n",
    "        def simple_tree():\n",
    "            return DecisionTreeClassifier(criterion = 'entropy', max_depth= 1)\n",
    "            \n",
    "        tree_dict = {}\n",
    "\n",
    "        tree1 = simple_tree()\n",
    "        tree1.fit(X1,y1)\n",
    "        print(\"threshold:\", tree1.tree_.threshold[0], \"feature:\", tree1.tree_.feature[0])\n",
    "\n",
    "        ### made up alpha, for example\n",
    "        alpha1 = .6\n",
    "        tree_dict[1] = (tree1, alpha1)\n",
    "\n",
    "        tree2 = simple_tree()\n",
    "        tree2.fit(X2,y2)\n",
    "        print(\"threshold:\", tree2.tree_.threshold[0], \"feature:\" ,tree2.tree_.feature[0])\n",
    "        \n",
    "        ### made up alpha, again.\n",
    "        alpha2 = .35\n",
    "        tree_dict[2] = (tree2, alpha2)\n",
    "    \n",
    "        print(predict(X, tree_dict))\n",
    "        #--> np.array([1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
    "        \n",
    "        ###############################\n",
    "        ### For Further Checking of your function:\n",
    "        ### The sum of predictions from the two models should be:\n",
    "        \n",
    "        # If tree2 splits on feature 1:\n",
    "        # np.array([ 0.25  0.25 -0.95 -0.95 -0.25 -0.95 -0.25 -0.25 -0.95 -0.25])\n",
    "        \n",
    "        # If tree2 splits on feature 0:\n",
    "        # np.array([ 0.95  0.95 -0.25 -0.25 -0.25 -0.25 -0.25 -0.25 -0.95 -0.95])\n",
    "        ###############################\n",
    "        \n",
    "    Assumptions:\n",
    "        The models in the `est-dict` tuple will return 0s and 1s.\n",
    "            HOWEVER, the prediction equation depends upon predictions\n",
    "            of -1s and 1s.\n",
    "            FINALLY, the returned predictions should be 0s and 1s.            \n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 9",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Census Data\n",
    "This assignment will use the [**`Census Income Data`**](https://archive.ics.uci.edu/ml/datasets/census+income) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.html). A thorough description of the data and its features may be accessed either at the link above, or [this text file](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names).  \n",
    "\n",
    "In particular, this classification attempts to predict whether or not a particular census respondant has an income of more or less than $50,000.  \n",
    "  \n",
    "#### The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "col_names = [\n",
    "\"age\", \"workclass\", \"fnlwgt\", \"education\",\n",
    "\"education-num\", \"marital-status\", \"occupation\", \"relationship\",\n",
    "\"race\", \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\",\n",
    "\"native-country\", \"income\"\n",
    "]\n",
    "\n",
    "data_path = \"../resource/asnlib/publicdata/adult.data\"\n",
    "\n",
    "data = pd.read_csv(data_path, header = None, names = col_names)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Taking a subset of the data relevant to the prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "cols = [\"age\", \"workclass\", \"education-num\", \"occupation\", \"sex\", \"hours-per-week\", \"income\"]\n",
    "data = data[cols]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "A quick look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i, col in enumerate(data.drop(\"hours-per-week\", axis = 1)):\n",
    "    d = data[col].value_counts().sort_index()\n",
    "    plt.bar(d.index, d)\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.title(col)\n",
    "    plt.show()\n",
    "\n",
    "plt.hist(data['hours-per-week'], bins = 20);\n",
    "plt.title(\"Hours Per Week\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### The target variable of income over/under $50k is _________\n",
    "### 'a') mostly balanced\n",
    "### 'b') unbalanced\n",
    "### Assign character associated with your choice as string to ans1\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 10",
     "locked": true,
     "points": "3",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Preprocessing\n",
    "The following cells  demonstrates the functions used in preprocessing, concluding with the division of data into a training and testing sets, that are then preprocessed.  \n",
    "\n",
    "##### Dummy Variables\n",
    "\n",
    "In the creation of dummy variables, the most frequently occuring class will be dropped as a way of avoiding multicollinearity.  \n",
    "\n",
    "Thus, for **`n`** categories, **`n-1`** features will be created. Below demonstrates finding the most frequent category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "val_count = data['workclass'].value_counts()\n",
    "print('All Value Counts:')\n",
    "print(val_count)\n",
    "\n",
    "top = val_count[0]\n",
    "print(\"\\nTop category:\", top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### In the above, the top category is obtained by making a reference\n",
    "### to the `.index` attribute.\n",
    "### What would result from instead calling val_count[0]\n",
    "\n",
    "### 'a') The observed frequency of that top term\n",
    "### 'b') The result would be the same\n",
    "### 'c') an error would be thrown: `.iloc[0]` is needed\n",
    "### Assign the character associated with your choice as a string to ans1\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "ans1 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 11",
     "locked": true,
     "points": "3",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Hot Encoder  \n",
    "\n",
    "Dummies will be created using `sklearn`'s `OneHotEncoder`.  The following cell demonstrates fitting and transforming data using the One Hot Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ex_df = pd.DataFrame(\n",
    "    np.array([['b', 'a', 'c', 'a', 'c',], [\"z\",\"y\",\"y\",\"y\",\"z\"]]).T,\n",
    "    columns = [\"beg\",\"end\"],\n",
    "    index = range(500,505))\n",
    "\n",
    "test_df = pd.DataFrame(\n",
    "    np.array([[\"c\",\"b\"],[\"y\",\"y\"]]).T,\n",
    "    columns = [\"beg\",\"end\"],\n",
    "    index = [56,72])\n",
    "\n",
    "print(\"Initial DataFrame:\")\n",
    "print(ex_df)\n",
    "\n",
    "print(\"\\nTest df\")\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate OneHotEncoder\n",
    "# sparse = False means data will not be stored in sparse matrix\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "\n",
    "# Fitting OHE with the \"training\" data\n",
    "ohe.fit(ex_df)\n",
    "\n",
    "# Transforming the \"training\" dat\n",
    "tr_vals = ohe.transform(ex_df)\n",
    "\n",
    "print(\"\\nTransformed values\")\n",
    "print(tr_vals)\n",
    "\n",
    "print(\"\\nCategories\")\n",
    "print(ohe.categories_)\n",
    "\n",
    "# Creating column names from `.categories_`\n",
    "ohe_cats = np.concatenate(ohe.categories_)\n",
    "\n",
    "# In creation of new df. Note the use of np.concatenate\n",
    "final_df = pd.DataFrame(tr_vals, columns = ohe_cats)\n",
    "\n",
    "print(\"\\nFinal DataFrame\")\n",
    "print(final_df)\n",
    "\n",
    "# Putting everything together to transform test data\n",
    "print(\"\\nTransformed test df\")\n",
    "print(pd.DataFrame(ohe.transform(test_df), columns= ohe_cats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### True or False:\n",
    "### The OneHotEncoder preserves the index of a DataFrame\n",
    "\n",
    "### Assign Boolean choice to ans1\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 12",
     "locked": true,
     "points": "3",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "##### LabelEncoder\n",
    "\n",
    "`sklearn`'s `LabelEncoder` will be used to transform our income variable from strings to 0s and 1s.  \n",
    "\n",
    "Demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create target Series\n",
    "target_train = pd.Series(np.random.choice(data['income'].unique(), size = 10))\n",
    "target_test = pd.Series(np.random.choice(data['income'].unique(), size = 5))\n",
    "print(\"Target Train Series\")\n",
    "print(target_train)\n",
    "\n",
    "print(\"\\nTarget Test Series\")\n",
    "print(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit with training data\n",
    "le.fit(target_train)\n",
    "\n",
    "# Transform training and test data\n",
    "trans_train = le.transform(target_train)\n",
    "trans_test = le.transform(target_test)\n",
    "\n",
    "print(\"Transformed training values\")\n",
    "print(trans_train)\n",
    "\n",
    "print(\"\\nTransformed test values\")\n",
    "print(trans_test)\n",
    "\n",
    "print(\"\\nLabelEncoder `.classes_`\")\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "##### Define a custom preprocessing function\n",
    "Using the processes demonstrated above, a function is created to preprocess the census data.  \n",
    "\n",
    "The function is then used to create a training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_census(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    ### Hardcode variables which need categorical encoding\n",
    "    to_encode = [\"workclass\", \"occupation\", \"sex\"]\n",
    "\n",
    "    ### Find top categories in categorical columns\n",
    "    ### Used for dropping majority class to prevent multi-colinearity\n",
    "    top_categories = []\n",
    "\n",
    "    for col in to_encode:\n",
    "        top_categories.append(X_train[col].value_counts().index[0])\n",
    "\n",
    "    ### Create and fit one-hot encoder for categoricals\n",
    "    OHE = OneHotEncoder(sparse = False)\n",
    "    OHE.fit(X_train[to_encode])\n",
    "\n",
    "    ## Create and fit Label encoder for target\n",
    "    LabEnc = LabelEncoder()\n",
    "    LabEnc.fit(y_train)\n",
    "\n",
    "    def create_encoded_df(X, to_encode = to_encode, OHE = OHE, top_categories = top_categories):\n",
    "        # Return columns which need encoding.\n",
    "        def return_encoded_cols(X, to_encode = to_encode, OHE = OHE, top_categories = top_categories):\n",
    "            # Use onehotencoder to transform.\n",
    "            # Use \"categories\" to name\n",
    "            toRet = pd.DataFrame(OHE.transform(X[to_encode]), columns = np.concatenate(OHE.categories_))\n",
    "\n",
    "            # Drop top_categories and return\n",
    "            return toRet.drop(top_categories, axis = 1)\n",
    "\n",
    "        # create encoded columns\n",
    "        ret_cols = return_encoded_cols(X)\n",
    "\n",
    "        # Drop columns that were encoded\n",
    "        dr_enc = X.drop(to_encode, axis = 1)\n",
    "\n",
    "        # Concatenate values\n",
    "        # use index from original data\n",
    "        # use combined column names\n",
    "        return pd.DataFrame(np.concatenate([ret_cols.values, dr_enc.values],axis = 1),\n",
    "                            index = dr_enc.index,\n",
    "                            columns = list(ret_cols.columns) + list(dr_enc.columns))\n",
    "\n",
    "\n",
    "    def encode_target(y, LabEnc = LabEnc):\n",
    "        # Use label encoder, and supply with original index\n",
    "        return pd.Series(LabEnc.transform(y), index= y.index)\n",
    "\n",
    "    return create_encoded_df(X_train), create_encoded_df(X_test), encode_target(y_train), encode_target(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and testing sets; preprocess them.\n",
    "target = data['income']\n",
    "predictors = data.drop(\"income\", axis = 'columns')\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_census(*train_test_split(predictors, target, test_size = .2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Fitting Models to Data  \n",
    "\n",
    "If the above functions are defined correctly, the following cells should work; creating predictions from your adaptive-boosted model.\n",
    "\n",
    "Try playing around with the number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "d = simple_adaboost_fit(X_train.values.copy(), y_train.values.copy(), 50)\n",
    "preds = predict_T(X_test, d)\n",
    "print(classification_report(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "This image gives an idea of how the precision and recall on both the training and test set changes as the number of estimators is changed.  \n",
    "\n",
    "![predictor](./assets/PreRedEst.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"sklearn\"></a>\n",
    "### `sklearn` Implementation of Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators = 50)\n",
    "RF.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest:\\n\")\n",
    "print(classification_report(y_test, RF.predict(X_test)))\n",
    "ABC = AdaBoostClassifier(n_estimators = 50)\n",
    "ABC.fit(X_train, y_train)\n",
    "print(\"\\nAdaBoost:\\n\")\n",
    "print(classification_report(y_test, ABC.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You should find that the precision and recall of the your custom Adaptive Boosting are very similar to the `sklearn` adaboost.\n",
    "\n",
    "Notice the higher precision of the AdaBoost compared to the Random Forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
