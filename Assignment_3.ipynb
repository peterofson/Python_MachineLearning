{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Bayesian Linear Regression  \n",
    "\n",
    "-----------\n",
    "\n",
    "_Author: Khal Makhoul, W.P.G.Peterson_  \n",
    "\n",
    "## Project Guide\n",
    "\n",
    "--------------\n",
    "\n",
    "- [Project Overview](#overview)\n",
    "- [Introduction and Review](#intro)\n",
    "- [Coding Bayesian Linear Regression](#code)\n",
    "- [Data Refresher](#data)\n",
    "\n",
    "<a id = \"overview\"></a>\n",
    "## Project Overview  \n",
    "\n",
    "------------\n",
    "#### EXPECTED TIME 3 HRS\n",
    "\n",
    "This assignment will test your abilities in two different sections: the [review](#intro) section will revisit Bayes' formula and evaluate your ability to calculate simple Bayesian posterior probabilities. The [coding](#code) section will ask you to build functions that calculate the parameters of Bayesian posteriors for Bayesian Linear Regression.  \n",
    "\n",
    "In a separate notebook, there is a brief demonstration of `pymc3`. The `pymc3` module is a standard for using Markov Chain Monte Carlo to estimate Bayesian derived distributions, in lieu of calculating exact integrals.  \n",
    "One note on `pymc3`: it may be tricky to make function on your individual machine\n",
    "\n",
    "Programming questions will include:\n",
    "- Calculation of Bayesian Posterior\n",
    "- Calculating the $w_{MAP}$\n",
    "- Estimating $\\sigma^2$\n",
    "- Calculating $\\Sigma$  \n",
    "\n",
    "**Motivation**: Bayesian regression allows us to quantify the uncertainty in our model building / the point estimates of our weights calculated in Least Squares Regression.  \n",
    "\n",
    "**Objectives**: This assignment will:\n",
    "- Test fundamental Bayesian knowledge, particulary in regard to Linear Regression\n",
    "- Introduce `pymc3`  \n",
    "\n",
    "**Problem**: Once again we will be using housing data to predict house price using living area and year built.  \n",
    "\n",
    "**Data**: Our data comes from [Kaggle's House Prices Dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). \n",
    "\n",
    "#### Imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell imports the necessary modules and sets a few plotting parameters for display\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "<a id = \"intro\"></a>\n",
    "### Introduction and Review.\n",
    "\n",
    "Bayesian Regression comes with a different toolset than ordinary Linear Regression. In turn, that toolset demands a slightly different mindset. We start with a short review to hightlight the ways in which Bayesian thinking proceeds.\n",
    "\n",
    "Consider a population whose age distribution is as follows:\n",
    "\n",
    "| Age group | $\\%$ of total population   |\n",
    "|------|------|\n",
    "|   $\\le 35$  | $25 \\%$|\n",
    "| $36-65$ | $45 \\%$ |\n",
    "| $\\ge 66$ | $30 \\%$|\n",
    "\n",
    "Say you know the following results of a study about YouTube viewing habits:\n",
    "\n",
    "| Age group | $\\%$ in this group that watch YouTube every day  |\n",
    "|------|------|\n",
    "|   $\\le 35$  | $90 \\%$|\n",
    "| $36-65$ | $50 \\%$  |\n",
    "| $\\ge 66$ | $10 \\%$ |\n",
    "\n",
    "**Prompt: If you know a user watches YouTube every day, what is the probability that they are under 35?**\n",
    "\n",
    "\n",
    "We will start with a prior, then update that prior using the likelihood and the normalization from Bayes's formula. We define the following notation:\n",
    "\n",
    "* $A$: YouTube watching habit\n",
    "* $B$: Age\n",
    "* $A = 1$: User watches YouTube every day \n",
    "* $A = 0$: User does not watch YouTube every day\n",
    "* $B \\le 35$: User has age between 0 and 35\n",
    "* $36 \\le B \\le 65$: User has age between 36 and 65\n",
    "* $B \\ge  66$: User has age greater than 65\n",
    "\n",
    "The prior can be read from the first table: \n",
    "\n",
    "$$P(B \\le 35) = 0.25$$\n",
    "\n",
    "We are looking to calculate the posterior probability:\n",
    "\n",
    "$$P(B \\le 35|A = 1)$$\n",
    "\n",
    "With Bayes's formula:\n",
    "\n",
    "$$P(B|A) = \\frac{P(A|B)P(B)}{P(A)}$$\n",
    "\n",
    "For our question:\n",
    "\n",
    "$$P(B \\le 35|A=1) = \\frac{P(A=1|B \\le 35)*P(B \\le 35)}{P(A=1)}$$  \n",
    "\n",
    "While the tables do not contain the value of $P(A=1)$ it may be calculated:  \n",
    "\n",
    "$P(A=1) = $  \n",
    "\n",
    "\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(A=1| B\\le 35)*P(B\\le 35) \\ + $  \n",
    "\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(A=1|35<B<65)* P(35<B<65)\\  + $  \n",
    "\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(A=1|B\\ge 65)*P(B\\ge 65)$\n",
    "\n",
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### In the example above, P(A=1|B<35) is the:\n",
    "### 'a')prior\n",
    "### 'b')liklihood\n",
    "### 'c')nomalization\n",
    "### 'd')posterior\n",
    "\n",
    "### assign character associated with your choice as string to ans1\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 1",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Given the values in the tables above, calculate the posterior for:\n",
    "\n",
    "### \"If you know a user watches Youtube every day,\n",
    "### what is the probability that they are under 35?\"\n",
    "\n",
    "### Assign float (posterior probability between 0 and 1) to ans1\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = 0.46875000000000006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46875000000000006"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Given A what is Prob(B) => Pr(B | A )\n",
    "    # Pr(B | A) = {Pr ( A |B ) * Pr(B)} / Pr(A) \n",
    "    x = .9*.25 + (.5*.45) + (.1*.3)\n",
    "    y = .9*.25\n",
    "    ans = y/x\n",
    "    ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 2",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called `calc_posterior`\n",
    "\n",
    "### ACCEPT three inputs\n",
    "### Two floats: the likelihood and the prior\n",
    "### One list of tuples, where each tuple has two values corresponding to:\n",
    "### ### ( P(Bn) , P(A|Bn) )\n",
    "### ### ### Assume the list of tuples accounts for all potential values of B\n",
    "### ### ### And that those values of B are all mutually exclusive.\n",
    "### The list of tuples allows for the calculation of normalization constant.\n",
    "\n",
    "### RETURN a float corresponding to the posterior probability\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def calc_posterior(likelihood, prior, norm_list):\n",
    "    lng = len(norm_list)\n",
    "    count = 0\n",
    "    my_denom = []\n",
    "    \n",
    "    while (count < lng):\n",
    "        for x in norm_list:\n",
    "            x = norm_list[count][0]\n",
    "            y = norm_list[count][1]\n",
    "            denom = x*y\n",
    "            my_denom.append(denom)\n",
    "            count=count+1\n",
    "    top = likelihood*prior\n",
    "    bottom = sum(my_denom)\n",
    "    result = top/bottom\n",
    "    return result \n",
    "    \"\"\"\n",
    "    Calculate the posterior probability given likelihood,\n",
    "    prior, and normalization\n",
    "    \n",
    "    Positional Arguments:\n",
    "        likelihood -- float, between 0 and 1\n",
    "        prior -- float, between 0 and 1\n",
    "        norm_list -- list of tuples, each tuple has two values\n",
    "            the first value corresponding to the probability of a value of \"b\"\n",
    "            the second value corresponding to the probability of \n",
    "                a value of \"a\" given that value of \"b\"\n",
    "    Example:\n",
    "        likelihood = .8\n",
    "        prior = .3\n",
    "        norm_list = [(.25 , .9), (.5, .5), (.25,.2)]\n",
    "        print(calc_posterior(likelihood, prior, norm_list))\n",
    "        # --> 0.45714285714285713\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45714285714285713\n"
     ]
    }
   ],
   "source": [
    "    # TESTING FOR ABOVE FUNCTION \n",
    "    likelihood = .8\n",
    "    prior = .3\n",
    "    norm_list = [(.25 , .9), (.5, .5), (.25,.2)]\n",
    "    print(calc_posterior(likelihood, prior, norm_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 3",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Applicability\n",
    "\n",
    "In real life, this situation corresponds to:  \n",
    "1. Surveying people and asking them two questions (what's your age? do you watch YouTube every day?), then tabulating the percentage of each age group that watch YouTube everyday.\n",
    "2. After having collected that data, observing the anonymized watching habits of a set of different users (not the survey takers) - without access to additional demographic info -  and using the above survey to derive a probability for the anonymized users' age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Bayesian Linear Regression\n",
    "\n",
    "In Bayesian Linear Regression, our prior expresses a belief about the parameters of linear regression we wish to calculate, namely that the linear coefficient vector should have a small absolute value, and that deviations from zero should be Gaussian. This prior is mathematically equivalent to the Ridge Regression condition.\n",
    "\n",
    "So the question we will now ask is: conditioned on the data, how does our belief regarding the parameters of linear regression change? \n",
    "\n",
    "This is the same prior-to-posterior calculation of the above exercise.  \n",
    "\n",
    "Bayes' Rule:\n",
    "\n",
    "$P(B|A) = \\frac{P(A|B)\\ P(B)}{P(A)}$,\n",
    "\n",
    "For linear Regression:\n",
    "\n",
    "$p(w | y, X) = \\frac{p(y | w, X)\\ p(w)}{p(y | X)}$  \n",
    "\n",
    "What do we know, and what do we not know?\n",
    "\n",
    "* $p(w) = N(0, \\lambda^{-1} I)$: That's the prior on $w$ --- Known.  \n",
    "\n",
    "\n",
    "* $p(y | w, X) = N(X w, \\sigma^2 I)$: That's the likelihood expression --- Known.  \n",
    "\n",
    "\n",
    "* $p(y | X)$: That's the marginal probability of $y$ --- NOT KNOWN\n",
    "\n",
    "Rewriting the marginal probability in detail, using an integral instead of a sum - since $w$ is a continuous variable.\n",
    "\n",
    "$p(y | X) = \\int_{\\mathbb{R^d}} p(y, w | X)\\ dw$\n",
    "\n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\int_{\\mathbb{R^d}}\\ p(y | w, X)\\  p(w)\\ dw$\n",
    "\n",
    "\n",
    "At this point approximation is frequently required as the above integral usually has no closed form.  \n",
    "\n",
    "<a id= \"code\"></a>\n",
    "\n",
    "### Coding Bayesian Linear Regression\n",
    "\n",
    "In lecture, we obtained an equation for the posterior probability of $w$, the linear regression parameter vector:\n",
    "\n",
    "$$p(w|y, X) = N(w|\\mu, \\Sigma)$$\n",
    "\n",
    "#### where\n",
    "\n",
    "$$\\Sigma = (\\lambda \\ I + \\sigma^{- 2}\\ X^T\\ X)^{−1}$$\n",
    "\n",
    "$$\\mu = (\\lambda \\ \\sigma^2 I + X^T\\ X)^{-1}\\ X^T y ⇐ w_{MAP}$$\n",
    "\n",
    "\n",
    "\n",
    "Recall that $\\sigma^2$ is a parameter characterizing the deviation of the data from the line defined by $Xw$. While we don't know the true underlying parameter, we can estimate it by using the empirical deviation:\n",
    "\n",
    "$$\\sigma^2 \\approx \\hat{\\sigma}^2 = \\frac{1}{n − d}\\Sigma_{i=1}^n ( y_i − X_i w )^2$$  \n",
    "Where $w$ in the above is the $w_{LeastSquares} = (X^T\\ X)^{-1}\\ X^T y$\n",
    "\n",
    "\n",
    "-----------------------\n",
    "When it comes to prediction:  \n",
    "$p( y_0|x_0,y,X) = N(y_0|\\mu_0,\\sigma^2_0)$  \n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mu_0 = x^T_0\\mu$  \n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma^2_0 = \\sigma^2 + x_0^T\\Sigma x_0$  \n",
    "\n",
    "-----------------------------\n",
    "This section will involve coding five functions:\n",
    "- `x_preprocess`\n",
    "- `calculate_map_coefficients`\n",
    "- `estimate_data_noise`\n",
    "- `calc_post_cov_mtx`\n",
    "- `predict` \n",
    "\n",
    "Such that the functions `fit_bayes_reg` and `predict_bayes_reg` - outlined below - will work correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_bayes_reg(input_x, output_y, lambda_param):\n",
    "    \n",
    "    # Ensure correct shape of X, add column of 1's for intercept\n",
    "    aug_x = x_preprocess(input_x) # <----\n",
    "    \n",
    "    # Calculate least-squares weights\n",
    "    ml_weights = calculate_map_coefficients(aug_x, output_y, 0, 0) # <----\n",
    "        \n",
    "    # Estimate sigma^2 from observations\n",
    "    sigma_squared = estimate_data_noise(aug_x, output_y, ml_weights) # <----\n",
    "    \n",
    "    # Calculate MAP weights\n",
    "    weights = calculate_map_coefficients(aug_x, output_y, lambda_param, sigma_squared) # <---- \n",
    "\n",
    "    \n",
    "    # Create posterior covariance matrix\n",
    "    big_sig = calc_post_cov_mtx(aug_x, sigma_squared, lambda_param) # <----\n",
    "    \n",
    "    return weights, big_sig\n",
    "\n",
    "def predict_bayes_reg(x_obs, weights, big_sig):\n",
    "    \n",
    "    # Ensure correct shape of X, add 1's for intercept\n",
    "    aug_x = x_preprocess(x_obs) # <----\n",
    "    \n",
    "    # find mean / variance parameters describing prediction for data\n",
    "    mu_0, sig_sq_0 = predict(aug_x, weights, big_sig) # <----\n",
    "    \n",
    "    return mu_0, sig_sq_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: X-matrix preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Build a function called \"x_preprocess\"\n",
    "### ACCEPT one input, a numpy array\n",
    "### ### Array may be one or two dimensions\n",
    "\n",
    "### If input is two dimensional, make sure there are more rows than columns\n",
    "### ### Then prepend a column of ones for intercept term\n",
    "### If input is one-dimensional, prepend a one\n",
    "\n",
    "### RETURN a numpy array, prepared as described above,\n",
    "### which is now ready for matrix multiplication with regression weights\n",
    "\n",
    "def x_preprocess(input_x):\n",
    "    if (input_x.ndim > 1):\n",
    "        if( input_x.shape[0] < input_x.shape[1]):\n",
    "            input_x = np.transpose(input_x)\n",
    "        prepend = np.ones(len(input_x)).reshape(len(input_x),1)\n",
    "        input_x = np.concatenate((prepend, input_x), axis = 1) \n",
    "    elif (input_x.ndim == 1):\n",
    "        input_x = np.insert(input_x,0,1)\n",
    "    \"\"\"\n",
    "    Reshape the input (if needed), and prepend a \"1\" to every observation\n",
    "    \n",
    "    Positional Argument:\n",
    "        input_x -- a numpy array, one- or two-dimensional\n",
    "    \n",
    "    Example:\n",
    "        input1 = np.array([[2,3,6,9],[4,5,7,10]])\n",
    "        input2 = np.array([2,3,6])\n",
    "        input3 = np.array([[2,4],[3,5],[6,7],[9,10]])\n",
    "        \n",
    "        for i in [input1, input2, input3]:\n",
    "            print(x_preprocess(i), \"\\n\")\n",
    "            \n",
    "        # -->[[ 1.  2.  4.]\n",
    "              [ 1.  3.  5.]\n",
    "              [ 1.  6.  7.]\n",
    "              [ 1.  9. 10.]] \n",
    "\n",
    "            [1 2 3 6] \n",
    "\n",
    "            [[ 1.  2.  4.]\n",
    "             [ 1.  3.  5.]\n",
    "             [ 1.  6.  7.]\n",
    "             [ 1.  9. 10.]] \n",
    "\n",
    "    Assumptions:\n",
    "        Assume that if the input is two dimensional, that the observations are more numerous\n",
    "        than the features, and thus, the observations should be the rows, and features then columns\n",
    "    \"\"\"\n",
    "    return input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  4.],\n",
       "       [ 1.,  3.,  5.],\n",
       "       [ 1.,  6.,  7.],\n",
       "       [ 1.,  9., 10.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            # *** TESTING FOR ABOVE FUNCTION *** # \n",
    "    #input_x = np.array([[2,3,6,9],[4,5,7,10]]) # TEST CASE 1: Cleared \n",
    "    #input_x = np.array([2,3,6]) # TEST CASE 2: cleared \n",
    "    #input_x = np.array([[2,4],[3,5],[6,7],[9,10]]) # TEST CASE 3 : Cleared\n",
    "        \n",
    "    if (input_x.ndim > 1):\n",
    "        if( input_x.shape[0] < input_x.shape[1]):\n",
    "            input_x = np.transpose(input_x)\n",
    "        prepend = np.ones(len(input_x)).reshape(len(input_x),1)\n",
    "        input_x = np.concatenate((prepend, input_x), axis = 1) \n",
    "    elif (input_x.ndim == 1):\n",
    "        input_x = np.insert(input_x,0,1)\n",
    "        \n",
    "    \n",
    "    input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 4",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 5: MAP Coefficients:\n",
    "\n",
    "$$\\mu = (\\lambda \\ \\sigma^2 I + X^T\\ X)^{-1}\\ X^T y = w_{MAP}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Build a function called `calculate_map_coefficients`\n",
    "\n",
    "### ACCEPT four inputs:\n",
    "### Two numpy arrays; an X-matrix and y-vector\n",
    "### Two positive numbers, a lambda parameter, and value for sigma^2\n",
    "\n",
    "### RETURN a 1-d numpy vector of weights.\n",
    "\n",
    "### ASSUME your x-matrix has been preprocessed:\n",
    "### observations are in rows, features in columns, and a column of 1's prepended.\n",
    "\n",
    "### Use the above equation to calculate the MAP weights.\n",
    "### ### This will involve creating the lambda matrix.\n",
    "### ### The MAP weights are equal to the Ridge Regression weights\n",
    "\n",
    "### NB: `.shape`, `np.matmul`, `np.linalg.inv`,\n",
    "### `np.ones`, `np.identity` and `np.transpose` will be valuable.\n",
    "\n",
    "### If either the \"sigma_squared\" or \"lambda_param\" are equal to 0, the return will be\n",
    "### equivalent to ordinary least squares.\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def calculate_map_coefficients(aug_x, output_y, lambda_param, sigma_squared):\n",
    "    lamb = np.identity(len(aug_x[0])) \n",
    "    # lambda*sigma_squared*I_matrix \n",
    "    first_var = (lambda_param*sigma_squared)*lamb\n",
    "    # xMatrix * xMatrix.T \n",
    "    second_var = aug_x.T@aug_x\n",
    "    # lambda*sigma_squared*I_matrix + xMatrix.T * xMatrix \n",
    "    part1 = np.linalg.inv(np.add(first_var,second_var))\n",
    "    # xMatrix.T*yVector \n",
    "    part2 = aug_x.T@output_y\n",
    "    coefs = part1@part2\n",
    "    if ((lambda_param == 0) or (sigma_squared == 0)):\n",
    "        coefs = np.linalg.inv(second_var)@part2\n",
    "    return coefs\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the maximum a posteriori LR parameters\n",
    "    \n",
    "     Positional arguments:\n",
    "        aug_x -- x-matrix of training input data, augmented with column of 1's\n",
    "        output_y -- vector of training output values\n",
    "        lambda_param -- positive number; lambda parameter that\n",
    "            controls how heavily to penalize large coefficient values\n",
    "        sigma_squared -- data noise estimate\n",
    "        \n",
    "    Example:\n",
    "        output_y = np.array([208500, 181500, 223500, \n",
    "                             140000, 250000, 143000, \n",
    "                             307000, 200000, 129900, \n",
    "                             118000])\n",
    "                             \n",
    "        aug_x = np. array([[   1., 1710., 2003.],\n",
    "                           [   1., 1262., 1976.],\n",
    "                           [   1., 1786., 2001.],\n",
    "                           [   1., 1717., 1915.],\n",
    "                           [   1., 2198., 2000.],\n",
    "                           [   1., 1362., 1993.],\n",
    "                           [   1., 1694., 2004.],\n",
    "                           [   1., 2090., 1973.],\n",
    "                           [   1., 1774., 1931.],\n",
    "                           [   1., 1077., 1939.]])\n",
    "                           \n",
    "        lambda_param = 0.01\n",
    "        \n",
    "        sigma_squared = 1000\n",
    "        \n",
    "        map_coef = calculate_map_coefficients(aug_x, output_y, \n",
    "                                             lambda_param, sigma_squared)\n",
    "                                             \n",
    "        ml_coef = calculate_map_coefficients(aug_x, output_y, 0,0)\n",
    "        \n",
    "        print(map_coef)\n",
    "        # --> np.array([-576.67947107   77.45913349   31.50189177])\n",
    "        \n",
    "        print(ml_coef)\n",
    "        #--> np.array([-2.29223802e+06  5.92536529e+01  1.20780450e+03])\n",
    "        \n",
    "    Assumptions:\n",
    "        -- output_y is a vector whose length is the same as the\n",
    "        number of rows in input_x\n",
    "        -- input_x has more observations than it does features.\n",
    "        -- lambda_param has a value greater than 0\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.29223802e+06,  5.92536529e+01,  1.20780450e+03])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "           ### TESTING \n",
    "    \n",
    "        output_y = np.array([208500, 181500, 223500, \n",
    "                             140000, 250000, 143000, \n",
    "                             307000, 200000, 129900, \n",
    "                             118000])\n",
    "                             \n",
    "        aug_x = np. array([[   1., 1710., 2003.],\n",
    "                           [   1., 1262., 1976.],\n",
    "                           [   1., 1786., 2001.],\n",
    "                           [   1., 1717., 1915.],\n",
    "                           [   1., 2198., 2000.],\n",
    "                           [   1., 1362., 1993.],\n",
    "                           [   1., 1694., 2004.],\n",
    "                           [   1., 2090., 1973.],\n",
    "                           [   1., 1774., 1931.],\n",
    "                           [   1., 1077., 1939.]])\n",
    "        lambda_param = 0.01\n",
    "        sigma_squared = 1000\n",
    "        \n",
    "        # Lambda matrix     \n",
    "        lamb = np.identity(len(aug_x[0])) \n",
    "        # lambda*sigma_squared*I_matrix \n",
    "        first_var = (lambda_param*sigma_squared)*lamb\n",
    "        # xMatrix * xMatrix.T \n",
    "        second_var = aug_x.T@aug_x\n",
    "        # lambda*sigma_squared*I_matrix + xMatrix.T * xMatrix \n",
    "        part1 = np.linalg.inv(np.add(first_var,second_var))\n",
    "        # xMatrix.T*yVector \n",
    "        part2 = aug_x.T@output_y\n",
    "        coefs = part1@part2\n",
    "\n",
    "        ml_coef = calculate_map_coefficients(aug_x, output_y, 0,0)\n",
    "        ml_coef\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 6: Estimate Data Noise\n",
    "$$\\sigma^2 \\approx \\hat{\\sigma}^2 = \\frac{1}{n − d}\\Sigma_{i=1}^n ( y_i − X_i w )^2$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called `esimate_data_noise`\n",
    "\n",
    "### ACCEPT three inputs, all numpy arrays\n",
    "### One matrix coresponding to the augmented x-matrix\n",
    "### Two vectors, one of the y-target, and one of ML weights.\n",
    "\n",
    "### RETURN the empirical data noise estimate: sigma^2. Calculated with equation given above.\n",
    "\n",
    "### NB: \"n\" is the number of observations in X (rows)\n",
    "### \"d\" is the number of features in aug_x (columns) \n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def estimate_data_noise(aug_x, output_y, weights):\n",
    "    n = aug_x.shape[0]\n",
    "    d = aug_x.shape[1]\n",
    "    # empty list \n",
    "    sqdiflist = []\n",
    "    # Xi*weights\n",
    "    xiweights = -1*aug_x@weights\n",
    "    # output_y - Xi*weights\n",
    "    diff = np.add(output_y,xiweights)\n",
    "    sqdiff = np.square(diff)\n",
    "    # Appending empty list \n",
    "    for x in sqdiff:\n",
    "        sqdiflist.append(x)\n",
    "    # Summing the terms \n",
    "    squares_summed = sum(sqdiflist)\n",
    "    # Finding sigma^2\n",
    "    noise = (1/(n-d))*squares_summed\n",
    "    return noise\n",
    "    \n",
    "        \n",
    "    \"\"\"Return empirical data noise estimate \\sigma^2\n",
    "    Use the LR weights in the equation supplied above\n",
    "    \n",
    "    Positional arguments:\n",
    "        aug_x -- matrix of training input data\n",
    "        output_y -- vector of training output values\n",
    "        weights -- vector of LR weights calculated from output_y and aug_x\n",
    "        \n",
    "        \n",
    "    Example:\n",
    "        output_y = np.array([208500, 181500, 223500, \n",
    "                                140000, 250000, 143000, \n",
    "                                307000, 200000, 129900, \n",
    "                                118000])\n",
    "        aug_x = np. array([[   1., 1710., 2003.],\n",
    "                           [   1., 1262., 1976.],\n",
    "                           [   1., 1786., 2001.],\n",
    "                           [   1., 1717., 1915.],\n",
    "                           [   1., 2198., 2000.],\n",
    "                           [   1., 1362., 1993.],\n",
    "                           [   1., 1694., 2004.],\n",
    "                           [   1., 2090., 1973.],\n",
    "                           [   1., 1774., 1931.],\n",
    "                           [   1., 1077., 1939.]])\n",
    "        \n",
    "        ml_weights = calculate_map_coefficients(aug_x, output_y, 0, 0)\n",
    "        \n",
    "        print(ml_weights)\n",
    "        # --> [-2.29223802e+06  5.92536529e+01  1.20780450e+03]\n",
    "        \n",
    "        sig2 = estimate_data_noise(aug_x, output_y, ml_weights)\n",
    "        print(sig2)\n",
    "        #--> 1471223687.1593\n",
    "        \n",
    "    Assumptions:\n",
    "        -- training_input_y is a vector whose length is the same as the\n",
    "        number of rows in training_x\n",
    "        -- input x has more observations than it does features.\n",
    "        -- lambda_param has a value greater than 0\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1471223687.1592972"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                    ### TESTING \n",
    "        output_y = np.array([208500, 181500, 223500, \n",
    "                                140000, 250000, 143000, \n",
    "                                307000, 200000, 129900, \n",
    "                                118000])\n",
    "        aug_x = np. array([[   1., 1710., 2003.],\n",
    "                           [   1., 1262., 1976.],\n",
    "                           [   1., 1786., 2001.],\n",
    "                           [   1., 1717., 1915.],\n",
    "                           [   1., 2198., 2000.],\n",
    "                           [   1., 1362., 1993.],\n",
    "                           [   1., 1694., 2004.],\n",
    "                           [   1., 2090., 1973.],\n",
    "                           [   1., 1774., 1931.],\n",
    "                           [   1., 1077., 1939.]])\n",
    "        \n",
    "        wts = np.array([-2.29223802e+06, 5.92536529e+01, 1.20780450e+03])\n",
    "        n = aug_x.shape[0]\n",
    "        d = aug_x.shape[1]\n",
    "        # empty list \n",
    "        sqdiflist = []\n",
    "        # Xi*weights\n",
    "        xiweights = -1*aug_x@wts\n",
    "        # output_y - Xi*weights\n",
    "        # output_y[0] = 208500 + xiweights[0]= -228318.139959 = -19818.13995899976\n",
    "        diff = np.add(output_y,xiweights)\n",
    "        sqdiff = np.square(diff)\n",
    "        for x in sqdiff:\n",
    "            sqdiflist.append(x)\n",
    "        squares_summed = sum(sqdiflist)\n",
    "        squares_summed\n",
    "        noise = (1/(n-d))*squares_summed\n",
    "        noise\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 7: Posterier Covariance  \n",
    "\n",
    "$$\\Sigma = (\\lambda \\ I + \\sigma^{- 2}\\ X^T\\ X)^{−1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called \"calc_post_cov_mtx\"\n",
    "### ACCEPT three inputs:\n",
    "### One numpy array for the augmented x-matrix\n",
    "### Two floats for sigma-squared and a lambda_param\n",
    "\n",
    "### Calculate the covariance matrix of the posterior (capital sigma), via equation given above.\n",
    "### RETURN that matrix.\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "def calc_post_cov_mtx(aug_x, sigma_squared, lambda_param):\n",
    "    # Create identity matrix with lambda coef applied \n",
    "    first_var = lambda_param*np.identity(len(aug_x[0]))\n",
    "        \n",
    "    # sigma^-2 * Xmatrix.transpose * Xmatrix \n",
    "    sec_var = (1/sigma_squared)*aug_x.T@aug_x\n",
    "        \n",
    "    final = np.add(first_var,sec_var)\n",
    "    big_sigma = np.linalg.inv(final)\n",
    "    return big_sigma\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the covariance of the posterior for Bayesian parameters\n",
    "    \n",
    "    Positional arguments:\n",
    "        aug_x -- matrix of training input data; preprocessed\n",
    "        sigma_squared -- estimation of sigma^2\n",
    "        lambda_param -- lambda parameter that controls how heavily\n",
    "        to penalize large weight values\n",
    "        \n",
    "    Example:\n",
    "        output_y = np.array([208500, 181500, 223500, \n",
    "                                140000, 250000, 143000, \n",
    "                                307000, 200000, 129900, \n",
    "                                118000])\n",
    "        aug_x = np. array([[   1., 1710., 2003.],\n",
    "                           [   1., 1262., 1976.],\n",
    "                           [   1., 1786., 2001.],\n",
    "                           [   1., 1717., 1915.],\n",
    "                           [   1., 2198., 2000.],\n",
    "                           [   1., 1362., 1993.],\n",
    "                           [   1., 1694., 2004.],\n",
    "                           [   1., 2090., 1973.],\n",
    "                           [   1., 1774., 1931.],\n",
    "                           [   1., 1077., 1939.]])\n",
    "        lambda_param = 0.01\n",
    "        \n",
    "        ml_weights = calculate_map_coefficients(aug_x, output_y,0,0)\n",
    "        \n",
    "        sigma_squared = estimate_data_noise(aug_x, output_y, ml_weights)\n",
    "        \n",
    "        print(calc_post_cov_mtx(aug_x, sigma_squared, lambda_param))\n",
    "        # --> [[ 9.99999874e+01 -1.95016334e-02 -2.48082095e-02]\n",
    "               [-1.95016334e-02  6.28700339e+01 -3.85675510e+01]\n",
    "               [-2.48082095e-02 -3.85675510e+01  5.10719826e+01]]\n",
    "\n",
    "    Assumptions:\n",
    "        -- training_input_y is a vector whose length is the same as the\n",
    "        number of rows in training_x\n",
    "        -- lambda_param has a value greater than 0\n",
    "    \n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.99999874e+01, -1.95016334e-02, -2.48082095e-02],\n",
       "       [-1.95016334e-02,  6.28700339e+01, -3.85675510e+01],\n",
       "       [-2.48082095e-02, -3.85675510e+01,  5.10719826e+01]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        ### TESTING \n",
    "        output_y = np.array([208500, 181500, 223500, \n",
    "                                140000, 250000, 143000, \n",
    "                                307000, 200000, 129900, \n",
    "                                118000])\n",
    "        aug_x = np. array([[   1., 1710., 2003.],\n",
    "                           [   1., 1262., 1976.],\n",
    "                           [   1., 1786., 2001.],\n",
    "                           [   1., 1717., 1915.],\n",
    "                           [   1., 2198., 2000.],\n",
    "                           [   1., 1362., 1993.],\n",
    "                           [   1., 1694., 2004.],\n",
    "                           [   1., 2090., 1973.],\n",
    "                           [   1., 1774., 1931.],\n",
    "                           [   1., 1077., 1939.]])\n",
    "        lambda_param = 0.01\n",
    "        # GIVENS \n",
    "        ml_weights = calculate_map_coefficients(aug_x, output_y,0,0)\n",
    "        sigma_squared = estimate_data_noise(aug_x, output_y, ml_weights)\n",
    "        \n",
    "        # Create identity matrix with lambda coef applied \n",
    "        first_var = lambda_param*np.identity(len(aug_x[0]))\n",
    "        \n",
    "        # sigma^-2 * Xmatrix.transpose * Xmatrix \n",
    "        sec_var = (1/sigma_squared)*aug_x.T@aug_x\n",
    "        \n",
    "        final = np.add(first_var,sec_var)\n",
    "        big_sigma = np.linalg.inv(final)\n",
    "        big_sigma\n",
    "        \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Now we have functions for\n",
    "$$\\Sigma = (\\lambda \\ I + \\sigma^{- 2}\\ X^T\\ X)^{−1}$$\n",
    "$$\\mu = w_{map}=(\\lambda\\ \\sigma^2 I+X^TX)^{-1}X^Ty$$\n",
    "\n",
    "And thus, $p(w|y, X) = N(w|\\mu, \\Sigma)$, the posterior distribution of the linear regression parameters may be described.\n",
    "\n",
    "When we want to predict an unknown value, $y_0$ given observations $x_0$:  \n",
    "$p( y_0|x_0,y,X) = N(y_0|\\mu_0,\\sigma^2_0)$  \n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mu_0 = x^T_0\\mu$  \n",
    "$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma^2_0 = \\sigma^2 + x_0^T\\Sigma x_0$  \n",
    "\n",
    "#### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Code a function called \"predict\"\n",
    "### ACCEPT four inputs, three numpy arrays, and one number:\n",
    "### A 1-dimensional array corresponding to an augmented_x vector.\n",
    "### A vector corresponding to the MAP weights, or \"mu\"\n",
    "### A square matrix for the \"big_sigma\" term\n",
    "### A positive number for the \"sigma_squared\" term\n",
    "\n",
    "### Using the above equations\n",
    "\n",
    "### RETURN mu_0 and sigma_squared_0 - a point estimate and variance\n",
    "### for the prediction for x.\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def predict( aug_x, weights, big_sig, sigma_squared):\n",
    "    \"\"\"\n",
    "    Calculate point estimates and uncertainty for new values of x\n",
    "    \n",
    "    Positional Arguments:\n",
    "        aug_x -- augmented matrix of observations for predictions\n",
    "        weights -- MAP weights calculated from Bayesian LR\n",
    "        big_sig -- The posterior covarience matrix, from Bayesian LR\n",
    "        sigma_squared -- The observed uncertainty in Bayesian LR\n",
    "        \n",
    "    Example:\n",
    "        output_y = np.array([208500, 181500, 223500, \n",
    "                                140000, 250000, 143000, \n",
    "                                307000, 200000, 129900, \n",
    "                                118000])\n",
    "                                \n",
    "        aug_x = np. array([[   1., 1710., 2003.],\n",
    "                           [   1., 1262., 1976.],\n",
    "                           [   1., 1786., 2001.],\n",
    "                           [   1., 1717., 1915.],\n",
    "                           [   1., 2198., 2000.],\n",
    "                           [   1., 1362., 1993.],\n",
    "                           [   1., 1694., 2004.],\n",
    "                           [   1., 2090., 1973.],\n",
    "                           [   1., 1774., 1931.],\n",
    "                           [   1., 1077., 1939.]])\n",
    "        lambda_param = 0.01\n",
    "        \n",
    "        ml_weights = calculate_map_coefficients(aug_x, output_y,0,0)\n",
    "        \n",
    "        sigma_squared = estimate_data_noise(aug_x, output_y, ml_weights)\n",
    "        \n",
    "        map_weights = calculate_map_coefficients(aug_x, output_y, lambda_param, sigma_squared)\n",
    "        \n",
    "        big_sig = calc_post_cov_mtx(aug_x, sigma_squared, lambda_param)\n",
    "        \n",
    "        to_pred2 = np.array([1,1700,1980])\n",
    "        \n",
    "        print(predict(to_pred2, map_weights, big_sig, sigma_squared))\n",
    "        #-->(158741.6306608729, 1593503867.9060116)\n",
    "        \n",
    "    \"\"\"\n",
    "    mu_0 = float()\n",
    "    sigma_squared_0 = float()\n",
    "    \n",
    "    return mu_0, sigma_squared_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 8",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<a id = \"data\"></a>\n",
    "### Data Refresher  \n",
    "\n",
    "Once again, we will be using the Bayesian regression functions on house price data. Observations from this data will be used to test the functions you have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Read in the data\n",
    "tr_path = '../resource/asnlib/publicdata/train.csv'\n",
    "data = pd.read_csv(tr_path)  \n",
    "\n",
    "### The .head() function shows the first few lines of data for perspecitve\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data.plot('GrLivArea', 'SalePrice', kind = 'scatter', marker = 'x');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data.plot('YearBuilt', 'SalePrice', kind = 'scatter', marker = 'x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Question 9: On Housing Data\n",
    "\n",
    "Use your functions above to return a $\\Sigma$ and $\\mu$ for our housing dataset.\n",
    "\n",
    "Use \"SalePrice\" as the target, and \"GrLivArea\" and \"YearBuilt\" as predictors. Keep \"GrLivArea\" and \"YearBuild\", in that order. (Order is important for grading.)  \n",
    "\n",
    "**USE ALL OBSERVATIONS IN `data` NOT JUST THE FIRST 100 AS IS DONE IN THE EXAMPLE**\n",
    "\n",
    "e.g.\n",
    "```\n",
    "input_x = data[['GrLivArea, 'YearBuilt']].values\n",
    "```\n",
    "\n",
    "Use .1 for $\\lambda$ \n",
    "\n",
    "Below, return the $\\mu$ vector to the variable \"mu\"  \n",
    "\n",
    "Return the $\\Sigma$ matrix to the variable \"big_sig\"\n",
    "\n",
    "Remember, the `fit_bayes_reg` function should work if you defined the above equations correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### Follow directions above\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "\"\"\"\n",
    "Example:\n",
    "    input_x = data[['GrLivArea','YearBuilt']].head(100).values\n",
    "    output_y = data['SalePrice'].head(100).values\n",
    "    lambda_param = .1\n",
    "    \n",
    "    < --- CODE BLOCK --->\n",
    "    \n",
    "    print(mu)\n",
    "    #--> np.array([2.10423243e-02, 4.10449281e+01, 4.22635006e+01])\n",
    "    print(big_sig)\n",
    "    #--> \n",
    "    np.array([[ 9.99999861e+00, -1.75179751e-03, -2.74204060e-03],\n",
    "              [-1.75179751e-03,  6.50420674e+00, -3.47271893e+00],\n",
    "              [-2.74204060e-03, -3.47271893e+00,  4.60297584e+00]])\n",
    "\"\"\"\n",
    "\n",
    "mu = np.array([])\n",
    "big_sig = np.array([[]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "Question 9",
     "locked": true,
     "points": "10",
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Remeber to check out the other notebook for a demonstration of `pymc3`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
